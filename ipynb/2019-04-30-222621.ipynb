{"cells":[{"cell_type":"markdown","metadata":{},"source":["PostId: 2019-04-30-222621\nTitle:DNNの情報理論的な論文を読む"]},{"cell_type":"markdown","metadata":{"updated_at":1556607962756},"source":["Tishbyが良く研究しているっぽいので、その辺を中心に読んでいく。\n\n## Deep Learning and the Information Bottleneck Principle\n\n[arxiv1503.02406: Deep Learning and the Information Bottleneck Principle (2015)](https://arxiv.org/abs/1503.02406)\n\n2015年。DNNの学習をIBの枠組みで見てみる。\n入力と出力のMutual Information（以下MI）とか、hidden layerとhidden layerとhiddenの間のMIだとかを見ていく。\n何かが分かったというよりは、IB的な枠組みで捉えなおすとこうなりますよね、という内容。\n\nCover and Thomasで出てくるRate distortion theoryとか十分統計量とかData proccessing inequalityとか、\n関連する話題が多くてなかなか面白い。"]},{"cell_type":"markdown","metadata":{"updated_at":1556607971352},"source":["## Opening the Black Box of Deep Neural Networks via Information\n\n[arxiv1703.00810: Opening the Black Box of Deep Neural Networks via Information](https://arxiv.org/abs/1703.00810)\n\n先ほどの2015年論文の続編。"]},{"cell_type":"markdown","metadata":{"updated_at":1556630279940},"source":["### 対象としているデータ\n\n3に具体的な相互情報量の計算方法が載っている。\n30個の離散値に離散化して計算、とかなかなか面白い。\n\nただ教師データが何なのかがいまいち分からない。\n12 binary inputsで、2Dのsphere(つまり円か？)の12 uniformly distributed pointsだと言っている。\nXはこの4096パターンがどうこう、と言っているが、良く分からない。\n4096とは12 binaryという事で2の12乗っぽいが…\n\n[ソースはあった](https://github.com/ravidziv/IDNNs)が、var_u.matという行列データを食わせているだけで、このデータの作り方は不明。\nうーん。\n\nKazhdan 2003を参考にしているっぽいなぁ。\n[Rotation Invariant Spherical Harmonic Representation of 3D Shape Descriptors (pdf)](https://www.cs.princeton.edu/~funk/sgp03.pdf) これか。\n軽く読んだ。\n\n3次元のモデルデータを、何らかのdescriptorにする事で検索とかしやすくする、という物。\n回転不変なdescriptorにする為、いろいろやって64*64の二次元gird上の値にしている（Figure 4.)\n\nラベルデータとしてはViewpointのオブジェクトのラベルか。\nこれがKazhdan 2003らしい。\n\nこれをこの論文の3.1に書いてあるように変形している訳だ。\nつまり、ラベルを、各グリッドが対象としているモデルのうち、平均よりスペクトルが大きいか？というのをラベルとしている。\n\n…ん？どういう事だ？つまり適当な3Dモデル一つを選んで、4096通りの2値の教師データを作ったという事か？\nめっちゃ少ないが大丈夫なのか。\nそしてこのモデルに使ったデータが元のViewpointのどのオブジェクトかはどこかに書いておいてくれないと、このデータ作り直せないじゃないか。\n再現実験でも同じ.matファイルを使いまわしているっぽいし、これは酷いなぁ。\n\nちなみに提供されてるスクリプトで再現できない、みたいな事を言っているブログがあった。\n[Failure to replicate Schwartz-Ziv and Tishby](https://planspace.org/20180213-failure_to_replicate_schwartz-ziv_and_tishby/)\n\nなお、さらに続きの実験している方はもうちょっとちゃんとした再現手順を書いている。\n[github: Code for On the Information Bottleneck Theory of Deep Learning](https://github.com/artemyk/ibsgd)\n\nただ以上の理解を元にすれば、ようするに4096通りのXとそれに応じた0か1のYがある訳で、これらのテーブルデータから離散的な同時分布が分かる訳だ。\nP(X, Y)が分かるのは理解した。"]},{"cell_type":"markdown","metadata":{"updated_at":1556630285264},"source":["### Tの分布とはなんぞや？\n\n問題はP(T, X)だな。TはXから一意に決まってしまうので、同時分布なんて無いように思えるが。（しいて言えばデルタ関数だが）\n\n解説とコードを眺めていると、i番目のアクティベーションの値のそれぞれを別々の実現値として分布を考えているように見えるな。\nそれっておかしくないか？例えば3番目のレイヤーの一番上のニューロンと二番目のニューロンのactivationの値は別の変数で、\n両者の関係はかなり謎だと思うのだが。\nそれを同じ確率変数の分布として計算した物にどういう意味があるんだろう？\n\nうーん、意味は分からんがコードは自分の理解と同じ事をしているように見えるな。\nこれは結構何をしているか怪しいような？この論文は結構引用されたりバズったりしてた印象だが…\n\nまあいい。あるレイヤーのそれぞれのアクティベーションの値をTiという確率変数のそれぞれの実現値とみなしてMIを計算している。"]},{"cell_type":"markdown","metadata":{"updated_at":1556630354564},"source":["### 学習の二つのフェーズの動画\n\n論文には二つの動画を見ろ、とリンクが貼ってある。\n二つ目のリンクはデッドリンクだった（なんなのだろう？）。\n\n- [The optimization process in the Information Plane - symmetric rule](https://www.youtube.com/watch?v=P1A1yNsxMjc)\n\nこれは確かに主張の通りの綺麗な動画が取れているな。\n動画の説明では色はどこのレイヤーかを表している、と言っている。\n\n### 同一レイヤーの個々のニューロンの解釈は無意味\n\n3.5の最後に、個々のニューロンの相関は無くなり、個々のニューロンのweightの解釈は無意味となる、と言っている。\nどの辺からこれが言えているかはよく分からないが、\nこれがXとTの相互情報量で個々のニューロンのactivationを同一の確率変数の分布の結果のように考えるjustificationとなっている気もする。\n説得力は微妙だが、根拠は一応ありそう（先に言ってよ、という気もするが）。"]},{"cell_type":"markdown","metadata":{"updated_at":1556630377477},"source":["### 3.7 Focker-Planck equationってなんぞ？\n\n拡散方程式の一種になる、という事で、制約条件的なのがついた形の知られている確率過程の問題になるらしい。\nエクセンダールを見直したが同じ式は見つからなかった。\nただブラウン運動と似ているので、そういう感じのものなのだろう。\n\n式11は衝撃的な内容で、こんな凄い事が本当に分かるの？と思ってしまうが、\nFocker-Planck equationを知らないので本当かどうかも分からない。仕方ない。\n\n感覚的にはそれぞれの場所でMaximum Entropyを解く感じになるので、式11的にそれぞれのエントロピーが下がるから学習しなくてはいけない分布もより単純になりそうな気はする。\n\nこの辺が本当だったらDeep LearningでDeepにする効果が分かったと言えちゃうよなぁ。\nその割には実験の再現データの作り方とか皆からもっと突っ込み入りそうな物だが。"]},{"cell_type":"markdown","metadata":{"updated_at":1556630389856},"source":["### 3.8 理論的なboundとの一致（が良く分からない）\n\nとりあえず理論的なtの分布が出るのはいい。\nで、これを収束した時の各hiddden layerの分布と比較する、なら話は分かるのだが、\nこの理論値にはベータというパラメータがある。\nモデルの複雑さと答えの正確さのトレードオフみたいなのを調整する値。\n\nで、このベータを、各hidden layerに最も近くなるように決める、で、決めたら近かった、と言っている。\nうーん、うん？みたいな分からなさがあるな。"]}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"lanbuage_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.11"}},"nbformat":4,"nbformat_minor":0}