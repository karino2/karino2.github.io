---
title: "PRMLの14章"
date: 2018-07-08 10:21:46
---

以前勉強会では飛ばしていいか、と思った14章（boostingとかアンサンブルの話）だが、やっぱやりたい気がしたので自分担当でやる事に。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0387310738&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

という事でここに予習内容とか書いていきます。

### ざっと最後まで読んでみた

comittee、adaboost、conditional mixtuer modelという内容。
決定木は思ったよりページ数無くて、mixtuer modelが多い。

gradient boostとかは無いのね。

adaboostはexponential errorからの解釈が書かれていて、ここの内容はなかなか良いが、最初のアルゴリズムの提示の方が先に出てくるのでこちらは分かりにくい。

幾つか重要な式変形が練習問題に回されているのでやっても良い気はする。

どうしよっかね。とりあえずこの本に沿ってまとめてみるか、それともboostingにフォーカスしてGBDTとかそのヒストグラムの話を掘り下げるか…

勉強会をどう進めるかはおいといて、ひとまず教科書の内容をまとめてみよう。

# 14.1 Bayesian model averagingとの違い

14.1では初めに、本章で扱う手法とBayesian model averagingの違いについて言及している。

結論としてはBayesian model averagingは、全部のデータがどれか一つのモデルから生成されていて（式14.6）、でもそれがどれかが分からない、データが増えればだんだんと一つに収束していく、というもの。

一方で複数のモデルをcombineする場合は、例えば式14.5の場合などでは、各データ点ごとに別々のlatent variableが考えられて、だからおのおののデータ点が別々の構成要素から生成される。

式の形は似てるけど、意味合いは違いますよ、という話っぽい。
