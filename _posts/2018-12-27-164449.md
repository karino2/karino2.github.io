---
title: "サイコロ本（Foundations of Statistical Natural Language Processing）、11章〜"
layout: page	
---

ファイルサイズが大きすぎ、とgithub APIに怒られたので、ブログ記事を分割。

- サイコロ本を読む、トップへ
   - [サイコロ本を読む、1〜10章](https://karino2.github.io/2018/11/07/195900.html)

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262133601&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

# 11章 Probabilistic Context Free Grammars

文法の話。最初の所で用語やノーテーションがいろいろ出てくるので簡単にメモを取っておこう。

**dominate**

非終端のノード、$$N^j$$が、$${ { w_a} \cdot { w_b } }$$にrewriteされる時、$$N^j$$は$${ { w_a} \cdot { w_b } }$$をdominateしている、という。

また単語をあらわに書かずにdominateしている範囲（位置）だけを問題にする時は、$${ N^j}_{ a b }$$と書く。

### モデルの3つの仮定

1. 場所不変: どこをdominateしているかでrewriteの確率は変わらない
2. 文脈自由: サブツリーの確率は、ツリーの他の部分に依存しない
3. 先祖自由: サブツリーの確率は誰を親に持つかに依らない

### Chomsky Normal Formの仮定

文法は以下のどちらか

- 　$${ N^i} \rightarrow { { N^j} { N^k} }$$　
- 　$${ N^i} \rightarrow { w^j}$$　


## 11.3 文字列の確率

アウトサイドとインサイドの確率の再帰的な計算の話が出てくる。

インサイドは見たまんまだけどアウトサイドがややこしいので軽くメモしておく。

### アルファの定義

アウトサイドの確率、アルファは、以下のような形。

$${ \alpha_j({ p, q})} = { P({ { { w_{ 1 (p-1) }}, { { N ^j}_{ p q }}, { w_{ (q+1) m  }}}} \mid G)}$$

$$N^1$$から始めて$${ N ^j}_{ p q }$$が生成される確率、という事。

下付きの添字のjは、Nの上付きのインデックスを表す。
Nの上付きのインデックスは、1以外には特別な順番はなくて単に非終端記号のidに過ぎない。

### 11.3.3のメモ

帰納的にもっとも確率のたかい文法木のパースを得る方法の話だが、ぱっと見ものすごい計算量に見える。

Inductionのステップが凄い大変そう。
例えば一番上の分割を計算するためには、全分割での$$\delta_i({ p, q})$$が必要になる。

これってまたその先で、全分割での$$\delta_i({ p, q})$$が再帰的に必要になるので、上の方は凄い計算量だと思うんだよなぁ。これって全非終端記号の中から最大の奴を毎回探さないといけない訳で。
ボトムアップで簡単に出来ないと辛いよなぁ。

### 11.3のテーブルを手計算してみる

11.3.4のトレーニングのところで、式11.26のpreterminalのケースの式で、ベータがなんなのか良く分からなくなってきた。$$\beta_j({ p, q})$$という時は、wは所与の物を使うという前提では無いのか？すると$$\beta_j({ h, h})$$にPを掛けているのはなんなのだろう？

11.3.1まで戻ってテーブル11.3を自分で計算してみる事にする。

まずはfigure 11.1の二通りになる、という事を確認する必要があるかな。
どうしたらいいんだろう？下から始めてみるか。

まずpreterminalは一対一に対応している。

訂正: この記述は間違いで、sawはNPの可能性があるのを見落としていた。
ただsawがNPだとVがなくなるのでVPが作れず、Sには出来ない。

![images/2018-12-27-164449/0000.jpg]({{"/assets/images/2018-12-27-164449/0000.jpg" | absolute_url}})

次に、ルールのうち一通りしか無い物を見てみる。SからのNP VPがまず一通り。
次にPが含まれているルールはPPからP NPだけだが、Pの後ろは一つしかNPが無いのでこのルールを適用するしか無い。

Vを含むルールもVPのルールしか無いが、これはNPの側がどなるかはこの時点では決まらない。

という事でこの時点で即座に決まるのは以下か。

![images/2018-12-27-164449/0001.jpg]({{"/assets/images/2018-12-27-164449/0001.jpg" | absolute_url}})

VPが右に来るルールはSのところしか無いので、一番左のNPがSの左に来るのもこの時点で確定しているか。まぁいい。

で、Vの上のVPのところのNPをどうするか、というのは二通り考えられる。
まず隣のNPがVPを作る場合。

![images/2018-12-27-164449/0002.jpg]({{"/assets/images/2018-12-27-164449/0002.jpg" | absolute_url}})

もう一つの場合は隣のNPはそのさらに隣のPPと先にNPを作る場合。

![images/2018-12-27-164449/0003.jpg]({{"/assets/images/2018-12-27-164449/0003.jpg" | absolute_url}})

ここを決めれば残りはどちらも一通りしか無くて、ツリーが決まる。これがFig 11.1の2つのツリーになる。

ここまでを考えてみると、隣同士のどれを取るか、の組み合わせ分だけツリーを考えるのがシンプルそう。
だが行き止まりになってる事もあって、その場合はそのツリーのそこから上の確率はゼロになる。これは$$\alpha_j({ p, q})$$がゼロになる、という事か。
$$\alpha_j({ p, q})$$を理解した。

さて、元の良く分からなかった11.26式を改めて見てみる。
$$\beta_j({ p, q})$$のwは所与じゃないのか？という話はやはり所与に見える。
この唐突に出てくる$$w^k$$は特定の単語を表すidだな。
つまりある単語が$$N^j$$で生成されるか、という話か。

$$P({ w_h} = { w^k})$$というのは観測結果の物だけ残す、と言ってる訳だな。
うーむ、なんかぼやっとするな。
もう少しベータの手計算を続けるか。

Table 11.3を作るにあたり、まずテーブルを書いてみよう。

![images/2018-12-27-164449/0004.jpg]({{"/assets/images/2018-12-27-164449/0004.jpg" | absolute_url}})

このテーブルは、縦がpを、横がqを表しているんだな。
pからqまで、という順番なので、対角成分より上だけ埋まる。

対角成分はpreterminalからterminalの生成なのですぐ埋まるな。埋めてみよう。

![images/2018-12-27-164449/0005.jpg]({{"/assets/images/2018-12-27-164449/0005.jpg" | absolute_url}})

$$\beta_V$$が1というのはどういう意味だろう？
Vというルールから何が生成されるのか、という事を表から見ると、sawだけだ。だから1なのか。

あるルールから何が生成されうるか、という選択肢は、全部確率を足すと1になるのだな。
ちょっと分かってきた気がする。

つまり$$\beta_j({ p, p})$$というのは、$$N^j$$から位置pにある単語が生成される確率だな。

さて、この表をさらに埋める事を考えてみよう。
例えば1, 4が空欄になってるので、どうして空欄になるかを埋めようとしてみる事で考える。

まず1から4までをなにかのルールから生成されてる、と考える。
すると最終的にはそれはpreterminalから生成されるはずなので、

- 1, 1
- 2, 2
- 3, 3
- 4, 4

は含んでいる。
さらにこのルールはチョムスキーの標準形なので、preterminal以外のルールは2つの非終端記号から出来る。

1, 4を埋めるためには、

- 1,3と4, 4
- 1, 2と3, 4
- 1, 1,と2, 4

のどれかである必要があるか。


![images/2018-12-27-164449/0006.jpg]({{"/assets/images/2018-12-27-164449/0006.jpg" | absolute_url}})

つまり赤を埋めるには、対応する色のどれかの組が必要。
なるほど、目的のセルを、row側とcol側に見ていったそれぞれが要るんだな。

例えばPが右に来るルールは無いから、幾つかのセルは確率がゼロなのが分かる。
それをバツで消してみる。
ついでにPPからPのセルも埋めてみる。

![images/2018-12-27-164449/0007.jpg]({{"/assets/images/2018-12-27-164449/0007.jpg" | absolute_url}})

一番右上のセルに注目しよう。
これは必ずNP VPなのだが、という事は下のどこかはVPじゃないと駄目、という事だよな。
で、左はNPだ。

VPのセルというのは左にVが必要。
そう考えると、一番右の列の、3から下はVPになりえない。
2がVPなのは決まるな。
すると1, 1と2, 5のセルを使うのは確定するのか。
そして2, 5のセルはVPなんだな。

では$$\beta_VP({ 2, 5})$$をどうやって求めるか考えてみよう。
ここを埋める方法は、

- 2, 2と3, 5
- 2, 3と4, 5
- 2, 4と5, 5

の3通りがある。これは2, 5と言った瞬間に機械的に決まるな。
一番上は3, 5が必要。
3, 5は二通りか。3,3と4, 5か、または3, 4と5, 5。
そして3, 4は無いから3, 3と4, 5か。

これはNPからNPとPPが出来るルールだな。

![images/2018-12-27-164449/0008.jpg]({{"/assets/images/2018-12-27-164449/0008.jpg" | absolute_url}})

2, 5に戻って2, 4と5, 5のケースを考えてたら、2, 4ってありえない気がしてきた。

まず3, 4がありえない。すると、2, 4というのは2, 3と4, 4になるが、4, 4はPなのでPが右側に出るルールが必要で、それは無い。

そうすると1, 4も無いんじゃないか？
1, 4は2, 4と3, 4が無いので、1, 3と 4, 4しか残らない。そしてこれも右側にPが出るルールが無いと駄目。という事はこのバツは上全部うめられるのか。

![images/2018-12-27-164449/0009.jpg]({{"/assets/images/2018-12-27-164449/0009.jpg" | absolute_url}})

あとは2, 3を埋めて2, 3のケースでの2, 5も求めれば2, 5は計算出来るな。
これは確かに帰納的になっている。

### 11.3.4のトレーニングの問題に戻る

さて、理解が進んだところで式11.26に戻ってみよう。
左辺で求めようとしているのは特定のpreterminalからある単語が生成される確率だ。

右辺の1つ目の式を見てみよう。
分子はh番目が$$N^j$$でdominateされる確率と、そのh番目をdominateする非終端記号から単語が生成される確率とその単語が目的の単語の同時確率となってる。
この同時確率は分かりにくいな。

$$P({ N^j} \rightarrow { w^k})$$と、$$P({ { { N^j} \rightarrow { w_h}}, { { w_h} = { w^k}}})$$の違いを考えよう。

前者は$$N^j$$から特定の単語が生成される確率だな。
これは何番目、とか関係なく、どこでもいいからこのルールが成立する確率。

後者は難しいな。アルファも掛けた式全体で、h番目の単語が$$N^j$$から生成れる確率と、そのh番目が単語$$w^k$$と一致する確率の積としいう意味だろう。

つまりアルファも掛けた分子全体で、h番目の単語が$$w^k$$で、それが$$N_j$$から生成される確率だ。

アルファを除くと、h番目のpreterminalが$$N_j$$だった時に、h番目の単語が$$w^k$$となる確率か。

それはようするに

- h番目が単語$$w^k$$なら$$P({ N^j} \rightarrow { w^k})$$
- h番目がそれ以外の単語なら0

となる何かか。わかった。つまりデルタ関数をかけた物だな。

では次の11.27を見てみよう。

分母は$$N^j$$が今対象としているセンテンスのどこかに現れる確率。

分子のシグマの中はh番目の単語が$$N^j$$から生成されて、それが$$w^k$$となる確率。

という事で11.27全体では、文章の中のどこかの$$N^j$$が、$$w^k$$を生成する確率、という意味になり、それが左辺の表すところであろう。（厳密には文がgivenの時の値であり、それが左辺の最尤推定値となっている、という事だろう）。

よし、分かった。

### 11.3.4のトレーニングのあらすじを考える

式11.26やそれに付随する式を理解出来たので、全体のあらすじを考えよう。

前項で、非終端記号と終端記号の生成確率がわかればパースが出来るようになった。

この項はデータからこれらの生成確率を学習したい、という話。
ただトレーニングデータに文法ラベルは無い。

そこで

1. まず文法の適当な遷移確率を所与としてパースを行う
2. 行われたパースの結果をカウントする事で遷移確率の更新を行う

を繰り返す事で学習を行っていこう、と考えているようだ。

パースは確率的になる、という理解。

で、それらを元に11.3.2でやった帰納的な計算方法で、アルファとベータが計算出来る。

このアルファとベータを使えば、11.26式のように生成確率の期待値を求める事が出来る。

ここでもととなってる11.25式を見ると、右辺では古い生成確率が使われている。
これらの古いパラメータを使って構造を決めて、その構造からパラメータを推計している。

もう少し右辺を見ると、分子は以下の3つの確率の積となってる。

1. $$N^j$$が$$N^r$$と$$N^s$$になる、というルールの確率
2. $$N^j$$自身が発生する確率
3. $$N^r$$と$$N^s$$で実際の単語列を説明出来る確率

つまり1のルールで文章が説明される寄与度みたいなのが求まってるのだろうな。
これは$$N^j$$が発生する確率も含んでるので、割る事で生成確率の期待値を得る事が出来る。

## 11章、なかなか難しかった！

やっと読み終わった！
なんか記号が何を意味するかとか理解するのに苦戦して、結構読むのたいへんだった。
一応理解出来た気がしているが、本当にちゃんと理解出来てるかは少し自信無い。

それにしても頑張って理解したあとの、最後のPCFGのInside-Outsideアルゴリズムの問題点は「つまりそれは全然駄目なんじゃないか？」という気がして辛い。

ただアイデアとしては面白いので、ちゃんと計算追っておいたら何かの役に立つ事もあるかもしれないな、という気分にはなれた。

# 12章 Probabilistic Parsing

確率的なパースの話。

12章は大きく12.1と12.2に分けられて、12.1はパース全般についての概念とかの話。12.2は具体的なパースのシステムを見ていく、という構成らしい。

まずは12.1から見ていこう。

### 12.1.1から12.1.3までの概要

ふんふん、と読んでいったら、何の話をしているのか分からなくなったので立ち止まってメモを書く。

12.1.1ではパースのdisambiguationにprobabilisticな要素を入れる、モチベーションなどを解説している。

12.1.2は教師データの話で、treebankと呼ぶらしく、有名なのはPenn Treebankという事、その実際の中身を見てみる、という話。

で、12.1.3はどいう風に確率を定義するか、という話をしている。
ここは良く分からなくなった所なので、少し詳しく。

まずセンテンスをgivenとしてツリーの確率をモデリングするのは、やってる人は居るがちょっとおかしい、と言っている。

で、treeからセンテンスが一意に作られるとすると、ツリーでセンテンスを表す事が出来るので、ツリーとセンテンスの同時確率が、センテンスになるツリーの確率に落ちる。

その後、この確率が分かれば、あるセンテンスに対応する全ツリーの確率を足し合わせるとセンテンスの確率が得られる、という話をしている。

そこから言語モデルがあればパースのツリーから何がもっとも適切なパースかを選べる、という話がある（が良く分からない、後で考えてみる）

### 12.1.3の、何故センテンスをgivenとモデリングしないか？

さきほどのメモで、センテンスをgivenとした確率というのはちょっと普通じゃない（a  little odd）、みたいな話をしてたが良く分からないので考えてみる。

まずパースの直接的な表現としてはむしろそっちの方が普通だ。だからわざわざ言及しているのだろう。

で、oddだと言ってる理由は、確率のgivenの方に特定のセンテンスが入る、というのが、普通のセッティングと違うからだ、と言っている。

個々のセンテンスごとに確率を定義するのではなく、もっとある種の共通の属性をもったセンテンスのグループに対して確率を定義したい、といっている。

いまいち良く分からない。

なおここで、言語モデル、という物の定義がさらっと出てくるが、これがちょっと普通と違うので注意が必要。

12.1.3では言語モデルとは「個々のツリーの確率」という事になる。
センテンスの確率じゃない。

で、この言語モデルを使う方が普通だと言ってる。これのどのへんが「more general class of data」なんだろう？
全然分からん。

### 言語モデルとパースの話

言語モデルをツリーの確率という所に気づけば、あとの議論は割とストレート。

ツリーの確率が分かればセンテンスの確率が分かる事、ツリーの確率が分かれば、センテンスがgivenなツリーの確率はセンテンスを産む全ツリーの中で一番大きい確率のツリーを選ぶだけな事、などが書いてある。

ツリーの候補を列挙する方法は？という気はするが。

なお、言語モデルがあればツリーをunambiguate出来るが、その逆は正しくないとの事。
ツリーがパース出来ても、ツリーの確率が分からない、という場合はあるとか。

ただツリーの確率をモデリングした方がパースの精度は上がる事が多いとも言っている。へー。

### 12.2の後半が辛い

ずーっと論文紹介みたいな感じで、いろいろなアイデアをちょろっと紹介する、がダラダラ続いて、読んでて辛い。

この説明だけでは詳細はどうせ分からないし、その割にはそれが延々と続く。
しかもどれもアドホックな部分が多くて、2019年に学ぶ物じゃないよなぁ、という印象が強い。

## 12章は後半はいまいちだった

12章はProbabilisticなパースの章で、前半で手法に依存しない一般的な話を、後半で具体的な手法を見るという話だったのだが、後半は単なる紹介みたいなのでいまいちだった。

前半はなかなか勉強になった。

# 13 text、wordのalignmentと翻訳

# 13.1 textのalignment

翻訳のペアで、どのテキストが対応してるか、単語が対応してるかを探す、という問題。

### 13.1の良く分からない所をメモしておく

13.1の後半は単なる論文のabstract紹介みたいになってて、良く分からない。
ただ元論文読もうという程の興味も惹かれないので、分からない所をメモして先に進む事に。

13.1.2のGale and Church、sの求め方がいまいち分からない。パラグラフの長さの差の二乗を使ってる、とあるが、具体的にはどういう式かね？
普通は平均との差の二乗和だと思うが、ミューを使うのかしら？

13.1.3のオフセットアラインメント。図13.4の意味が分からない。

2つのテキストをつなげる、とあるが、短文同士を一つだけつなげてグラフを描く、という事か？
何故それで黒い四角の所が一様っぽくなるのだろう？

4gramの一致を見るなら対角線は左上から右下じゃなくて左下から右上になりそうだが。
例えばxが5と完全に一致するのはyが5の時に思えるが、何故逆なのだろう？

text alignmentの情報としては明るい象限の方が大切だ、と言ってるが、何故だろう？明るい象限は自身のテキストとのマッチングだから無価値なのでは？

2つのよりかすかな対角線が二本ある、というがどこの事だろう？そんなの見えないが。

Fung and McKeownの方法。
arival vectorとして262, 4, 252と書いてあるが、この意味が分からない。
大きさでソートされてないがこの順番にはどういう意味が？

## 13.1は良く分からないのが多かった

全体的にアブスト程度の説明でこれじゃ分からんな、という手法の列挙だった。
コアとなるアイデアくらい知っとこうと読んだが、分かった気はしなかった。

ただテキストアラインメントという問題自体があんま興味が湧かないのでまぁいっか。
現代的にはななめ読みで飛ばしてしまう方が良い気がする（なんとなく惰性で全部読んだが）

## 13.3 machine translationの話もいまいち

英語からフランス語を翻訳して、そのフランス語がもっとも出てきそうな英語を推測する、という話をしている。

戻した英語でロスを作る、という話では無い模様。

translation modelは単語同士の条件付き確率だけを元に、それの積を全アラインメントについて足し合わせる、みたいなモデル。（13.5）

これじゃ全然駄目じゃん、という気はするが、そういう時代なのだろう。

で、単語の条件付き確率はEM法で対訳文から求める。

具体的に書ける程は理解出来てないが、いかにも駄目そうなので軽くなめた程度で次に進んでしまって良い気がする。
2000年の時点では応用は遠いねぇ。

## 13はいまいちだった

読み終わったので感想とか。
あまり具体的な話がなくて、論文が列挙されてるだけの所が多い。
具体的な話がある所もあまり詳しくなくてちゃんとは理解出来なかった。

翻訳周りの具体的な話がいっぱい出てくるのを期待して読んだのだが、そういう章ではなかった。13章以降は実際の応用のパートという事になってるので、ここから先はこの程度の話しか無いのかも。

次以降もこんな感じならさらっと斜め読みして終わらせてしまおう。

ただ、ここまでやってきた事との対応みたいなのは眺める価値はあった。
ここで挙げられてる事も、該当論文読んでやれば出来そうだな、という気にはなったし、ここまでやってきた事の位置づけは分かる。

それにしても2000年の時点では翻訳は全然ダメそうだな。
単に単語置き換えるだけのシステムの方がマシなんじゃないか？と思えるほど。
ただ凝った事をやろうとして、かえっていまいちになる、という事を繰り返してマシな物に近づいていったのだろうとは思うので、そうした軌跡の記録にはなっている。

ただこの時代なら自分は自然言語処理はやらないなぁ。

# 14章 クラスタリング

途中まで読んでみたが、なんかNLPと関係ない普通のクラスタリングの話が多くて、そこはあまり読む意義が無さそう。

軽く要点だけおさえて先に行きたいな。

14.1がhierarchical clusteringで、14.2がflatなclusteringとか。

## 14.1 hierarchical clustering

bottom-upとtop-downのアルゴリズムがある。

bottom-upはagglomerative clusteringとも言うらしい。agglomerativeはくっつくとかそんな感じのニュアンスの模様。
bottom-up全体をそう呼ぶのか、具体例として挙げたFig.14.2のアルゴリズムをそう呼ぶかは分からんが、わりとどうでも良かろう。

bottom-upの例であるFig.14.2は、最初は全部にばらばらのクラスタを割り当てて、一番近い2つをまとめる、を繰り返すっぽい。

Fig.14.3はtop-downの例。
全部入ったクラスタから始めて、もっともcoherentで無いクラスタを分割する、を繰り返す。
coherentの具体例はここでは述べてない。

### 14.1.1 single-linkとcomplete-link

simにはsingle-linkとcomplete-linkとgroup-averageの3つがある、とか言ってる。
へー、初耳。

これはクラスタ同士のsimilarityみたいだね。
以前そのネタで論文を書いた事がある身としては興味深い。結構頑張って調べたがあんまこの手の議論は無いのだよなぁ。

single-linkは2つのクラスタのうち、一番近い要素同士のsimを使うっぽい。
completeは一番遠い物同士。

### 14.1.2 Group-avarage agglomerative clustering

重心同士の類似度をクラスタの類似度とする、という話。
重心の更新が高速に出来るから良い、という話。

別段どうという事は無いな。
そのほか言語モデルを改善するのに使う話（ただし改善幅は微妙…）とか、
top-downはあんま使われないが良い場合もある、とかいろいろ話はしてるが、どれもそんな難しい事は無い（というか問題が難しすぎて大した事は出来ない、というか）

## 14.2 Non-hierarchicalなクラスタリング

大した話は無いが、MDL（Maximum Description Length）という指標でクラスタ数を決める、というのは面白いな。

## 14章、流し読みする分には悪くなかった

ちゃんと理解しよう、とか考えずにサラサラ読んで、ぱっと見て分からない事は素直にあきらめて進むと、14章はなかなか悪くなかった。

EM法の解説は読み物レベルではわかりやすかったし、昔真面目にやった事をそれなりに思い出せて定着度が増した気がする。

その他いにしえのクラスタリング関連の参考文献とか多くて、必要になった時に資料として価値もありそう。

パート4はこの位の気持ちで読むのが良さそうね。サラサラ読んで先に進もう！

# 15章 Information Retrieval

この章も流し読みで行こう。
ただ最近勉強会でやってるトピックはこれなので、知りたい所ではある。

### 15.1.2 Evaluation measure

15.1.2のEvaluation measureは重要そうなので少し真面目に読む。

uninterpolated average precisionとinterpolated average precisionの2つが紹介されてる。
uninterpolatedは普通の平均。

interpolatedは、まずターゲットとするrecallの値を選び、それを実現出来るところまでランキングを下げていく。
その後最初のランキングから、precisionが上がる間（つまり正解が続く間）はランキングを下げた、最後の所のprecisionが定義らしい。

15.2はtf-idfの詳しい解説。
全部覚えるような話では無いと思うが、何かあった時にこのページ開こう、と思わせる資料的価値は感じた。

ノーテーション。$$cf_i$$はcollection frequencyで、単語$$w_i$$がどれだけ対象となるコーパス全体で出てきたか、という数。

15.3はポワソン分布の話。
復習としてちょうど良さそうなのでここは真面目にやるかなぁ。

我らのケースでは、ある単語$$w_i$$が一つのドキュメントにk回現れる確率、としてポワソン分布を使う。

パラメータは平均なので、一つのドキュメントにその単語が平均何回出てくるか、となる。つまり以下。

$${ \lambda_i} = \frac{ cf_i}{N}$$

### RIDFの計算

15.3.5で、insuranceとtryのRIDFを計算してみよう、と書いてあるのでしてみる。

Nは79291で、テーブル15.4は以下。

![images/2018-12-27-164449/0010.jpg]({{"/assets/images/2018-12-27-164449/0010.jpg" | absolute_url}})

RIDFの定義などは以下。

![images/2018-12-27-164449/0011.jpg]({{"/assets/images/2018-12-27-164449/0011.jpg" | absolute_url}})

まずはinsuranceから見てみよう。
ひたすら必要な物を計算していく。

![images/2018-12-27-164449/0012.jpg]({{"/assets/images/2018-12-27-164449/0012.jpg" | absolute_url}})

あってそう。
やれば出来そうなのでtryの方はいいか。

### 15.4以降

Latent semantic indexはSVDとかの話。
さすがにもうこれは読まなくていいだろう。

15.5のdiscourse segmentationはトピックこどに文章のブロックを分割する、という話。
TextTilingという手法が紹介されてて、まぁ普通にアドホックに考えてもこんな感じでやりそう、という内容。
一応目は通しておく。

## 15章はあまり知らない事は無かった

もともとこの辺はやってた事もあるので知らない事は少なかった。
たださらっと読むには悪くない。
目を通すくらいで先に進むのが良いね。

