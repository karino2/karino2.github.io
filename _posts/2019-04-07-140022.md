---
title: "MacKayのInformation Theory, Inference, and Learning Algorithms"
layout: page	
---

勉強会で読む事にしたMacKayの情報理論の本。Cover and Thomasと並びよく参照される情報理論の定番教科書。
こちらにはベイズと機械学習の話題がある。

Kindle版は無いが、pdfがある。

[Information Theory, Inference, and Learning Algorithms](http://www.inference.org.uk/itprnn/book.html)



# 一章

最初にスターリンの公式と二項分布がある。
準備体操に手を動かしてみるか。

### Example 1.1 偏りありのコイン投げ

表かfのコイン投げ。N回投げて表がr回出る確率と、rの平均と分散を求める。

まずは表がrの確率。

![images/2019-04-07-140022/0000.jpg]({{"/assets/images/2019-04-07-140022/0000.jpg" | absolute_url}})

右下にごみが入ってるが気にしない。
次は平均。

![images/2019-04-07-140022/0001.png]({{"/assets/images/2019-04-07-140022/0001.png" | absolute_url}})

これどう計算するんだっけ。二項定理使って微分すれば出そう？

![images/2019-04-07-140022/0002.png]({{"/assets/images/2019-04-07-140022/0002.png" | absolute_url}})

ふむ、Nfか。まぁそりゃそうだな。解説見ても一回の平均がfだからN回でNf、としてる。

次は分散。

![images/2019-04-07-140022/0003.png]({{"/assets/images/2019-04-07-140022/0003.png" | absolute_url}})

rの二乗の期待値を求める問題に帰着された。試行が独立なら別々の期待値の和だよな。r二乗の期待値は...1を二乗しても1だからfか？

すると、$$Nf-Nf^2$$だから、$$Nf(1-f)$$か。

### スターリンの公式の話

コンビネーションの計算をしておく。

![images/2019-04-07-140022/0004.jpg]({{"/assets/images/2019-04-07-140022/0004.jpg" | absolute_url}})

### binary entropy function

Cover and Thomasでかなり混乱した所なので、少しメモを書いておく。

1.14で定義されるbinary entropy functionは、普通のエントロピーの定義とすごく似ているが、引数が違う。

普通エントロピーは確率変数に対して定義される概念だが、このbinary entropy functionはベルヌーイ分布のパラメータの関数になってる。

もちろんこのパラメータで分布は決まり、その分布に従って求めたエントロピーにはなってるのだが、気をつけないとすぐに良くわからなくなるのでメモを書いておく。

### 講義の動画

以前寝ながらこの教科書の講義を見てた事があるが、内容はだいたい同じだな。

[youtube: Introduction to Information
Theory](https://youtu.be/BCiZc0n6COY)

## 1章読み終わり

ハミングコードとshanonのlimitの話で終わり。
この辺はまぁだいたい分かってるのでバンバン進もう。

# 2章、確率、エントロピー、推論

序盤は確率論の話。
ensembleというのは初耳だが、離散的な確率空間のうち、事象がアルファベットのみの物っぽい。

### 2.44式はどういう定義か？

Hの分解について書いてあるが、右側の定義が良く分からない。

![images/2019-04-07-140022/0005.jpg]({{"/assets/images/2019-04-07-140022/0005.jpg" | absolute_url}})

例えばmまでの和が3/4、そこから先の和が1/4とすると、定義はこうか？

![images/2019-04-07-140022/0006.jpg]({{"/assets/images/2019-04-07-140022/0006.jpg" | absolute_url}})

なんかこれっぽい気がするな。
記号は不思議な感じだが、1か2か3か…かmのどれかなら1、それ以外なら0の確率変数のエントロピー、のようなものか。

離散的な分布の時にprobability massを引数に列挙する、というノーテーションを導入してる訳か。なるほど。

# 3章 Inferenceについて

ちゃっちゃか進もう。

最初になんか解いてみろ、と書いてあるので、3.1を解いてみる。

### Ex 3.1 二種類の20面ダイス

まだ何の章なのかも良く分かってないが、とりあえず解いてみよう。

![images/2019-04-07-140022/0007.jpg]({{"/assets/images/2019-04-07-140022/0007.jpg" | absolute_url}})

うーむ、なんか別段新しい事も無いような。
この問題の主旨はなんだろう？

### Ex 3.3 decay定数の推計

このあとでこの問についてのエピソードから本題が始まってるっぽいのでこの問題だけ解いておく。

![images/2019-04-07-140022/0008.png]({{"/assets/images/2019-04-07-140022/0008.png" | absolute_url}})

事前分布がわからんな。どうなってるんだろう？
このくらいやっておけば本文は理解出来そうなので本文に戻るか。

![images/2019-04-07-140022/0009.jpg]({{"/assets/images/2019-04-07-140022/0009.jpg" | absolute_url}})

## 3.1はベイジアンの正当性の話

読んでいったら、ベイジアンへの批判とかについての話だった。
もうそういうのはいいよ、と思ってる身としては読み飛ばす事にする。

## 3.3 model comparison

3.2は3.3のためのお膳立てくらいであまりノートをとる事は無い。
3.3は仮説H1とH2のどちらがいいか、を知るためのmodel comparisonの話が。

PRMLでやったし大してわからん事も無いが、エビデンスの定義を書いておく。

データDがあった時に、

$$P(D |\mathcal{H}_1) $$

をモデルエビデンスと呼ぶ。
観測されたデータがそのモデルをどのくらい好むか、という値。

