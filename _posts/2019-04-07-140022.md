---
title: "MacKayのInformation Theory, Inference, and Learning Algorithms"
layout: page	
---

勉強会で読む事にしたMacKayの情報理論の本。Cover and Thomasと並びよく参照される情報理論の定番教科書。
こちらにはベイズと機械学習の話題がある。

Kindle版は無いが、pdfがある。

[Information Theory, Inference, and Learning Algorithms](http://www.inference.org.uk/itprnn/book.html)



# 一章

最初にスターリンの公式と二項分布がある。
準備体操に手を動かしてみるか。

### Example 1.1 偏りありのコイン投げ

表かfのコイン投げ。N回投げて表がr回出る確率と、rの平均と分散を求める。

まずは表がrの確率。

![images/2019-04-07-140022/0000.jpg]({{"/assets/images/2019-04-07-140022/0000.jpg" | absolute_url}})

右下にごみが入ってるが気にしない。
次は平均。

![images/2019-04-07-140022/0001.png]({{"/assets/images/2019-04-07-140022/0001.png" | absolute_url}})

これどう計算するんだっけ。二項定理使って微分すれば出そう？

![images/2019-04-07-140022/0002.png]({{"/assets/images/2019-04-07-140022/0002.png" | absolute_url}})

ふむ、Nfか。まぁそりゃそうだな。解説見ても一回の平均がfだからN回でNf、としてる。

次は分散。

![images/2019-04-07-140022/0003.png]({{"/assets/images/2019-04-07-140022/0003.png" | absolute_url}})

rの二乗の期待値を求める問題に帰着された。試行が独立なら別々の期待値の和だよな。r二乗の期待値は...1を二乗しても1だからfか？

すると、$$Nf-Nf^2$$だから、$$Nf(1-f)$$か。

### スターリンの公式の話

コンビネーションの計算をしておく。

![images/2019-04-07-140022/0004.jpg]({{"/assets/images/2019-04-07-140022/0004.jpg" | absolute_url}})

### binary entropy function

Cover and Thomasでかなり混乱した所なので、少しメモを書いておく。

1.14で定義されるbinary entropy functionは、普通のエントロピーの定義とすごく似ているが、引数が違う。

普通エントロピーは確率変数に対して定義される概念だが、このbinary entropy functionはベルヌーイ分布のパラメータの関数になってる。

もちろんこのパラメータで分布は決まり、その分布に従って求めたエントロピーにはなってるのだが、気をつけないとすぐに良くわからなくなるのでメモを書いておく。

### 講義の動画

以前寝ながらこの教科書の講義を見てた事があるが、内容はだいたい同じだな。

[youtube: Introduction to Information
Theory](https://youtu.be/BCiZc0n6COY)

## 1章読み終わり

ハミングコードとshanonのlimitの話で終わり。
この辺はまぁだいたい分かってるのでバンバン進もう。

# 2章、確率、エントロピー、推論

序盤は確率論の話。
ensembleというのは初耳だが、離散的な確率空間のうち、事象がアルファベットのみの物っぽい。

### 2.44式はどういう定義か？

Hの分解について書いてあるが、右側の定義が良く分からない。

![images/2019-04-07-140022/0005.jpg]({{"/assets/images/2019-04-07-140022/0005.jpg" | absolute_url}})

例えばmまでの和が3/4、そこから先の和が1/4とすると、定義はこうか？

![images/2019-04-07-140022/0006.jpg]({{"/assets/images/2019-04-07-140022/0006.jpg" | absolute_url}})

なんかこれっぽい気がするな。
記号は不思議な感じだが、1か2か3か…かmのどれかなら1、それ以外なら0の確率変数のエントロピー、のようなものか。

離散的な分布の時にprobability massを引数に列挙する、というノーテーションを導入してる訳か。なるほど。

# 3章 Inferenceについて

ちゃっちゃか進もう。

最初になんか解いてみろ、と書いてあるので、3.1を解いてみる。

### Ex 3.1 二種類の20面ダイス

まだ何の章なのかも良く分かってないが、とりあえず解いてみよう。

![images/2019-04-07-140022/0007.jpg]({{"/assets/images/2019-04-07-140022/0007.jpg" | absolute_url}})

うーむ、なんか別段新しい事も無いような。
この問題の主旨はなんだろう？

### Ex 3.3 decay定数の推計

このあとでこの問についてのエピソードから本題が始まってるっぽいのでこの問題だけ解いておく。

![images/2019-04-07-140022/0008.png]({{"/assets/images/2019-04-07-140022/0008.png" | absolute_url}})

事前分布がわからんな。どうなってるんだろう？
このくらいやっておけば本文は理解出来そうなので本文に戻るか。

![images/2019-04-07-140022/0009.jpg]({{"/assets/images/2019-04-07-140022/0009.jpg" | absolute_url}})

### 3章はいらなさそう

3.1を読んでいったら、ベイジアンへの批判とかについての話だった。
もうそういうのはいいよ、と思ってる身としては読み飛ばす事にする。

3.2は3.3のためのお膳立てくらいであまりノートをとる事は無い。

3.3はmodel comparison。
仮説H1とH2のどちらがいいか、を知るための話。

PRMLでやったし大してわからん事も無いが、エビデンスの定義を書いておいて先に進む。

他特に読む事は無さそうなので3章は飛ばす。

### エビデンス

3.3であったエビデンス、すぐには定義が思い浮かばなかったのでメモしておく。

データDがあった時に、

$$P(D |\mathcal{H}_1) $$

をモデルエビデンスと呼ぶ。
観測されたデータがそのモデルをどのくらい好むか、という値。

# 4章、The Source Coding Theorem

最初にex 2.21-2.25をやれ、と書いてあるが、単なる期待値計算とかで別段難しい事も無い。

とりあえず読む前に解けといってるEx 4.1を考えてみよう。

## 練習4.1 一つだけ変なボールと秤

問題を定式化してみよう。
秤はランダム変数Yとして、左が重いと0、右が重いと1とする。

個々のボールも確率変数とするか。
X1からX12。で、値は-1か0か1を取る。
11個の0と一つの-1か1の値、という制約条件。
ベクトルとしてはXと書く（フリック入力の都合）

どのボールが変かは分からないのでuniformなpriorとしよう。
うーむ、そうすると確率変数は、
どのボールが変かを表す方が良いか？
確率変数Tでどのボールがおかしいかを表すとしよう。これは1から12のどれかの値をuniformに取る。

さらにおかしいのが軽いか重いかを表す確率変数Wもあるとする。軽いと0、重いと1だとするか。

どの確率変数を使うかはわからんが、この位で進めてみよう。

### 何も試す前のエントロピー（試行0と呼ぶ）

知りたいのはTとWだな。
秤で測る結果との相互情報量みたいな概念にできそう？

とりあえずH(T, W)について考えよう。
これは、

![images/2019-04-07-140022/0010.png]({{"/assets/images/2019-04-07-140022/0010.png" | absolute_url}})

で良さそうか。

### 試行1-1 6個ずつかけてみる

うーん、どう考えたらいいんだろう。
まず適当に天秤にかけてみるか。
まず6個6個で天秤にかけるとする。
1, 2, 3, 4, 5, 6を左、7以降を右でかける。

左が重かったとしよう。
可能性としては、

- Wが重くて（1）、Tは左
- Wが軽くて（0）、Tは右

の2つに絞られた。
場合の数は6+6で12で、ぜんぶ確率は同じか。

この時のエントロピーを計算してみよう。

![images/2019-04-07-140022/0011.png]({{"/assets/images/2019-04-07-140022/0011.png" | absolute_url}})

ちなみにこの場合は右が重くてもまったく同じだな。
これをゼロにすれば良さそう。 

### 試行2-1 左の6個を3-3に分けて掛ける

最善かはおいといて、次は1から6を半分に分けて秤にかければ、Tは1から6か、7から12かのどちらから判明するな。
特定のWについて、p(T)がゼロじゃないのを集めて半分に分けて測っていけばだいたいわかりそう。

だが、これは最適じゃなさそうだな。
というのは、ハズレた時に反対側の方の情報がいまいち得られないから。

### 試行2-2 偶数番目を左、奇数番目を右に載せる（6-6で測る）

では、例えば次に、左に1, 3, 5, 7, 9, 11を、右に2, 4, 6, 8, 10, 12を乗せるとしよう。

今度は右が重かったとする。
Wは相変わらずどちらかは分からない。

- Wが重い(1)、重いのは2, 4, 6のどれか
- Wが軽い(0)、軽いのは7, 9, 11のどれか

この実験ならどっちが重くてもエントロピーは変わらなさそう。この場合のエントロピーを求めてみよう。

場合の数は6通りで全部確率は等しそう。

![images/2019-04-07-140022/0012.png]({{"/assets/images/2019-04-07-140022/0012.png" | absolute_url}})

### 試行2-1を計算してみる

では先程の、1から6を半分に分けるのを試行2-1と呼び、試行2-2とどっちがエントロピーを下げてるか計算してみよう。

試行2-1は、

- 左が下がる
- 右が下がる
- 釣り合う

の三通りが考えられる。
それぞれどういう場合が考えられるだろう？

- Wが重くてTが1, 2, 3か、Wが軽くてTが4, 5, 6
- Wが重くてTが4, 5, 6か、Wが軽くてTが1, 2,3
- Wがそれぞれの場合でTが7以降

これは試行1と矛盾してるのがありそうか？

Wが軽いとTは7以降だから、Tが7以降はWが軽い場合に限られる。

Tが6以下ならWは重い場合に限られる。
以上を考慮に入れると、

- Wが重くてTが1, 2, 3
- Wが重くてTが4, 5, 6
- Wが軽くてTが7以降

になる。場合の数は12通りか。

### 試行1の改善を考える

エントロピーの期待値を計算してみてもいいのだが、その前に、秤が必ず釣り合う試行はいまいちな気がしてきたな。
感覚的にはYの系列は毎回なるべく3通りの確率が同じになる方が良さそうだよな。

最初の試行に戻り、12個のボールでYが三通りに等しい確率となるためにはどう載せたらいいか？

どっちかに傾く確率と傾かない確率を1/3で分かれるようにする為には、傾かない確率が1/3なら良さそうか。
これはTが除けた方にはいる確率が1/3ならいいのだから、4つ除けたら良さそうか？

だから1〜4を左、5〜8を右、9〜12を載せない、としよう。これを試行1-2と呼ぶ事にする。

これを定式化すると、T, WのjointなエントロピーとYのエントロピーを考えて一番減るYを設計するのが良さそう。 
つまり、T, WとYの相互情報量を最大化すれば良さそう？

T, WとYのベン図を描いて考えると、、、こうか？

![images/2019-04-07-140022/0013.png]({{"/assets/images/2019-04-07-140022/0013.png" | absolute_url}})

つまり、Yがgivenの時のエントロピーを最小化すればいいのか。

