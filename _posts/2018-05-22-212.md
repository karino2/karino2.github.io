---
title: "GoodfellowのDeepLearning本を読む"
date: 2018-05-22 01:44:27
---

プログラム言語の本もだいたい読んだので、次は機械学習の本でも読むかな、と思い、Goodfellow本を読む事にする。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262035618&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

ちょうど年末に買っておいたので。

### どう読むか

この本は多分Deep Learningをバリバリやってる人向けでは無く、これからやっていきたいという人向けにいろいろ紹介していく、という物と自分は思ってる。
Deep Learningという物を良く知らない人が基礎から理解していき、最新の研究の入り口くらいまでの話題をいろいろ知る、という本なんじゃないか。
（まだ読んでないので予想だけど）

一方で自分は画像認識に関してはプロフェッショナルといえる水準だと思う。
実サービスで使っていく難しさとか自分たちなりの解決など、十分な経験がある。
だからこの本が目的としている、「読者をここまで連れていきたい」という場所よりは、既に先に居るんじゃないか。

この本が出たあとの論文もいろいろ読んでいるし、モデルもいろいろ触っている。だから自分にとっては、いろいろなトピックや研究を知る、という目的でこの本を読むのは、いまさらな感じがする。
変化が早い分野だからねぇ…。

目次を読む限り、トビックとしてはPart 3の知らない所だけ読めば事足りそうだが、それではたぶんあんまり読む事は無い。それが一番効率的な読み方かもしれないけど、どうせ暇なので、もうちょっと違う所を読んでみたいな。

ということで、せっかくGoogfellow御大が本を書いてくれたのだから、彼がニューラルネットをどう捉えているのか、みたいな、著者個人の感性みたいなのに注目して読んで行こうかなぁ、と思う。うまく行くかは分からんが。

逆に読者がDeep Learningを学ぶ為に必要、という感じの記述はガンガン飛ばして行きたい。
例えば今更Drop outとかBatch normarizationなどの話は読まなくて良かろう。

# Introduction

自分の目的を思うと、このintroductionとかのポエムパートの方がむしろ重要かもしれない。

MLPがrepresentation learningのような物だ、というのはいいとして、
マルチステップのプログラムを学習しているような物と見れる、というのは面白い。
確かに中間レイヤのWが何に使われているのか、というのは、多様な見方を身に着けておく方が良い気はする。

### 想定読者

どちらも自分は入って無さそう。
その辺はそんな気はしてたので、自分に必要なように読んでいきたい。

### history

brainの模倣の路線では無くなった理由、とかは興味深いな。

wax and wane 盛衰。月の満ち欠け。waxに月が満ちるって意味があるのね。

### Introduction読み終わり

これが書かれた時とその後のトレンドは結構違うな、と思う。

ネットワークは最近はむしろ小さくなっているし、タスクが汎用になっているよりはより難しいタスクに挑む傾向がある気がする。

ある時点で振り返った時に続いている傾向がその後も続く訳では無い、というのは、重要な洞察だよな。

ただある一時点で過去からの流れをこうしてまとめて見る事は、そういった一歩引いた視点で物を考えるのには有用な気がする。

なかなか良く書けてて面白い導入だった。

# Part 1 Applied MathとMLの基礎

ここはタイトルからすると飛ばし読みでいいかなぁ、という気がするが、軽く見てから考えよう。

## 2章、線形代数

あつかってるトピックを眺めていく

- 行列、テンソルの定義
- 行列積、Identity、Inverse
-  Linear dependenceとspan

spanとはなんぞや？という事で2.4は軽く見てみる。
基底とか線形結合の話か。
この辺は日本語で勉強したので英単語知らないから、単語の勉強として軽く読む。
linear dependenceは線形従属の事やね。

そのあと特異値分解とかpinvが雑に紹介されてる。この辺をラプラス展開からちゃんとやらないのは気に食わないが、そういう本だという事だ。

reciprocalは逆数。良く出てくるがすぐ忘れる単語。

Frobeeius norm がトレースで表せる、という式2.49、見た覚えあるが、成り立ってるか軽く試す。

![](https://i.imgur.com/x3pDBXr.png)

成り立ちそうやね。

### 2章読み終わり

線形代数の提示の仕方としてはあまり好きになれない内容だが、線形代数の本じゃないのでこの辺はどうでも良い。

## 3章 確率論と情報理論

確率論は割と本気でやった領域なのでこの本で何か知る事があるとも思えない、という事で軽く流す。

最初に確率論ちゃんと勉強するならJaynes 2003を読め、と書いてある。

これか。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0521592712&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

amazon.com側ではレビューがいっぱいついてて、ベイズ統計の教科書っぽい。
jpでは一件もレビューがついてないのは、きっとみんなGoodfellowが提示してるリファレンスなんて見ても居ないからに違いない。

目次が読みたかったのでKindleにサンプル送ろうとしたら、おまえの国には送らん、と言われる。ぐぬぬ。

[本家のサイト](http://www.cambridge.org/gb/academic/subjects/physics/theoretical-physics-and-mathematical-physics/probability-theory-logic-science?format=HB&isbn=9780521592710#VQP13Vzbv7tYKUI5.97)はGoogle previewというので目次が見えるな。

ベイズ統計メインでタイトルのように論理的な推論の拡張として考える話があるが、それ以外のトピックもかなり多様に見える。

decision theoryとか結構長い。
毎回ちらっといろんな本に出てくるが何を言いたいのか良く分かってないので一度やってみても良いかもしらんが、まぁ今はいい。

数学的には初等的な範囲に見える。
あくまでベイジアンな応用をちゃんと扱う、という感じで、解析的な扱いはあまりして無さそう。

なかなか本格的な本ではあるが、さすがに生成モデル全盛の今の時代にこのレベルでは辛いのではなかろうか。

### 確率変数のあたり

3.1でfrequentistとbeyesian論争的なポエムがしばらく書かれたあとに、3.2で確率変数が出てくる。

「A random variable is a variable that can take on different values randomly.」
 
えっ？まじ！？

ってこの説明ではさすがにディープラーニングの話は出来なく無いか？
でも一パラグラフで確率変数の話を終えて次の3.3に進んでしまっている。

Goodfellow先生は確率論の解析的な話とかは全然重視てしないのかなぁ、数学なんか詳しくなくてもepoc makingな論文は書ける！という事なのだろうか。
さすがGoodfellow先生ほどの実績を残す男は違うな…  
ふむむ、考えさせられる。

と思い先を読むと、3.3.1でProbability Mass Functionという名前で結構突っ込んだ分布の確率測度の話が、そうした学術用語を出さずに続く。

えー、そりゃ無いよ！Goodfellow先生！

これはこんな書き方では測度論知らない人は何を言ってるのかすら理解出来ないだろう…

random variabieのstateとか完全に未定義な単語でいろいろ語るが、どう見てもボレル集合族の元の話だし、PMFが可測関数だって話だよなぁ、これ。

うぉ、そのあとに確率測度の満たす性質でもっとあらわに書き始めた！酷い開き直り！

うーん、ここまで開き直るくらいなら、最初からボレル集合族という言葉を出す方がずっと誠実だと思うんだが…

追記: 3.12に測度論の話あり。そちらに3.3の感想を追加。

### 初歩的な確率の関係

3.4から3.7まででマージナライズや積の法則や条件つき確率などの基本的な話を短くしてある。

この辺は確率論を忘れてる人が思い出す為、という感じで、初めて学ぶ人が読む物では無いし、この辺普段から使ってる人が読む物でも無い。

### 3.8 期待値とかcovとか

期待値はGANとかではかなり込み入った話となるので、注意して読もう、と思ったが、これがまた短い。
ただ普通の入門書の期待値よりは記法の話が細かい。これは必要だからだよな。

covの定義などの書き方も、統計の入門書よりは、より論文的になっている。

そのあと独立とcovがゼロの関係とかの話がダラダラとされている。この辺の突然素人向けの注意が挟まれるのはDeep Learningという言葉に寄ってくる人の性質を表しているなぁ。

### 3.9 良く出てくる分布

ベルヌーイ分布やガウス分布はおなじみだが、ポワソン分布やガンマ分布が出てこないのがディープラーニングやね。

やはりガウス分布の扱いは長い。

そして指数分布やらプラス分布、ディラック分布などはなかなからしさがある。
emprical distributionも機械学習らしさがあるね。

混合分布も意外と真面目に扱ってる。Gaussian mixtureとかやるの？
16.5で構造推定みたいなのやるらしい。
なんかPGMっぽいなぁ。

### 3.10 sigoidとsoft plusの性質

soft plusってあんまり知らないのでこの機会に覚えておく。
ついでにシグモイド関数の微分とかの式を眺めておく。

### 3.12には測度論の話がある

3.12まで読むと、先程まじかよ、って思った3.3.2は真面目にやると測度論が要る、という話をする。

だが3.3.2のどの辺が測度論が分かってないと良く分からないのか、という話はせず、機械学習の対象では病的な奴は出てこない、という話がある。
また、むしろ証明などを記述する道具として便利だ、と述べている。
この見解は自分と同じなので納得は出来る所。

でも3.3.2の話を測度論がわかる人向けにしないっていうのはどうなんだろう？これでは測度論を知らない読者が、その必要性を知る事は出来ない。

### 確率論の話を読んでの雑感

自分は実解析を最近かなり真面目にやってたのでその周辺の話を。

この本は、測度論の話を出す時に、測度論の用語を使わずに書く。
その結果、おそらく測度論を知らない人は、たまに良く分からない記述が混ざるが、別段分からない事は無い、と感じるんじゃないか。

でもこのたまに良く分からない所が混ざる、という所が後ろの方ではたぶん分からない元になるはずと思う。

少なくとも自分はかつて、後ろの方のトピックの元論文を読んだ時に良く分からないからどんどん遡っていった結果実解析をやり、その結果これまで分からなかった事がいろいろ分かるようになった。
だからまだの人はきっと私とは逆向きに同じ過程をたどるんじゃないか、と思う。

ただ、この本ではその分からない理由が意図的にわかりにくく書かれているように見える。
それは何故だろうか？

まず一番ポジティブな解釈としては、本当にこの辺の事を知らない人でも、だいたいは理解出来るように書ける、と思って書いた可能性。
私は3章の終わり時点までで既にそれは失敗していると思うが、
本当にそれを目指して書いているなら、応援したい気持ちはある。
たぶん誰かがそれをやれば大分関連分野の数学的敷居は大きく下がる、と自分も長らく思っていた所なので。

一番ネガティブな解釈は、どうせDeep Learningとかいって騒いでる連中のほとんどは内容が理解出来る訳では無いので、そういう連中は間違って買ってしまえばよろしい、と思ってそういう本に仕立てた、という事。
さすがにこれは無いか。

現実的には、これで分かると思って書いているが全然分かんねーよ、ってあたりな可能性はある。
Goodfellowが確率の根本原理に詳しすぎる結果、3.3の書き方で自然とボレル集合族的な物が感覚的に分かってしまうので、皆も分かるだろう、と思ってるとか。
これはありそうだなぁ…

さて、著者の意図はおいといて、読者の我々はどう向き合うべきか、という事を考えよう。

ここまで読んだ限り、必要な事に関しては、分かっている人が読めば分かるが、分からない人が読むと良く分からないように、という形で、一通り書いてあるように見える。

だから自分が分かっている分野に関しては、「これとこれとこれを分かってればいいのね、おっけー」と思って読めば良さそう。

問題は自分の良く知らない分野だ。
自分の良く知らない分野で、何を言ってるのか良く分からないが、なんかどうでも良さそうな事を言ってるのを見つけた時。

これはたぶん分かる人向けのメッセージなので、たぶん分かる方が良い。
特にあとの方で関連しそうな所でさっぱり分からなくなった時は、ここに立ち戻って、その関連分野を真面目に学ぶ必要があるのだろう。

だから、どうでも良さそうだが何を言ってるか分からない物を見つけたら要注意だ。ブックマークなりこのブログにメモなり残して、あとで分からない所が出てきた時にそこ由来かチェックしよう。

後半の内容読んでみないとここまでの印象があってるかは分からんので、読み進めてまた感想書く。

### 3.13 Information Theory

この節は以前職場にこの本が置いてあって、立ち読みした事がある。

自分はこの分野はやった事無い。
ガラケー屋だった時にCDMAなどでこの辺の話をちょっとやったのと、あとはKLダイバージェンスやエントロピーを使ってる、くらい。

PRMLでもこの本でも情報理論に言及があるので、真面目にやった方が良いのかもしれない。

最初に紹介されている本はCover and Thomas (2006)とMacKay (2003)らしい。

Cover Thomas 2006はこれで、

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0471241954&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

MacKey 2003はこれか。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0521642981&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

MacKeyの方が自分には合ってそうだが、本家のサイトにも電子版が無い…  
うーん、スキャンするのかったるいなぁ。どうしたもんか。

さて、内容に移る。
エントロピーがその分布をエンコードする下限のビット数の期待値となる、というのは自分は理解してない所で、
どうでも良く見えるがこれは知ってる人向けのメッセージなのだろうな。

KLダイバージェンスの追加に必要なうんたら、というくだりも良く分かってない。

KLダイバージェンスと非対称性の図3.6は、まったく同じものをPRMLで見た気がする。

良く忘れるのでKLダイバージェンスの式は書いておくか。p72。

![](https://i.imgur.com/sdTkGHm.jpg)

クロスエントロピーとかの話はPRMLのEM法で結構やったので理解出来てると思う。

### 3.14 Structured Probabilistic Models

この辺は超得意分野だぜ〜

- [PGMコースの受講メモ](http://jbbs.shitaraba.net/bbs/read.cgi/study/12706/1466006216/)
- [PRML勉強会、36〜60](Pattern Recognition and Machine Learning
http://jbbs.shitaraba.net/bbs/read.cgi/study/12706/1481675604/36-60)

以前junction treeアルゴリズムとかをちゃんと理解する為に、以下の本と、

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4320111397&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

あとDaphneの本も買って、面白そうな所だけつまみ読みした。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262013193&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

内容としてもベイジアンネットとマルコフネットの話で何も分からん事は無い、という感じ。
Part3で突っ込んだ話をする、との事で楽しみだ。

# 4章、数値計算

ペラペラめくった所、grad descentとかキューンタッカーの定理とかの話っぽい。
飛ばすか悩むが、軽く読んでおくか。

### 4.3.1 JacobianとHessian行列

4.3のgrad descentの説明まではさらさらと読んで行った。で、4.3.1のHessian。

HessianはPRMLで出てきた時に、結局何に使うのか解説されてなくて良くわからんなー、と思ってた所なので真面目に読んで見る。

Hessianが対称とかgradのヤコビアンだ、というあたりまではいい。

そのあと、単位ベクトルdによるsecond derivativeが $$d^TH\ d$$で表せる、みたいな話は知らない話だな。

軽く手を動かしておこう。
まず1階の微分から。p82に載ってるが、この手のは久しぶりにやる時は添字をサボらないに尽きる。

簡単の為、R2からRへの射影の場合をやろう。

![](https://i.imgur.com/mZW5olY.jpg)

よし、だいたい思い出した。
では二階の微分をやってみよう。

![](https://i.imgur.com/bevYxhn.jpg)

![](https://i.imgur.com/GkorLu1.jpg)

![](https://i.imgur.com/YXzmnvW.jpg)

そのあとdがヘッシアンの固有ベクトルなら二階の微分が固有値になる、というのは、簡単に出せるな。

![](https://i.imgur.com/F8fwr7t.jpg)

固有ベクトルじゃない時は、固有値の重み付け和になる、という話だが、これは対象とする方向ベクトルを固有ベクトルの線形結合で書けばそうなるだろう。

で、この方向ベクトルを変えていって二階の微分最大の場所は固有値最大の固有ベクトルの方向になる。
これもほぼ明らかだね。

さて、これを使ってのstep sizeの決定。
4.10は4.9をイプシロンについて平方完成すれば出そう。

そしてgradを先程の固有ベクトルの線形結合で表す議論を思い出せば、ラムダmaxの逆数になるのも明らか。

次にこのHessianの使い方として、ヘッシアンの固有値が大きく違う方向があると、片方で適切なラーニングレートではもう片方にとって適切でなく、いったりきたりを繰り返して無駄に進むケースの図がある。

そして二次の項まで考えたニュートン法の式がある。

この辺の話は使うのかね？ 
そのあとに鞍点ではgrad descentに劣る、とか書いてあるな。やっぱり使わなくない？

### リプシッツ定数の説明が！

そのあとに唐突にリプシッツ連続とリプシッツ定数の説明が。
この辺に触れてるのはDeep Learningっぽいね！後半どう使うのか期待！

そのあとにはconvexなケースの話があるが、Deep Learningには関係ないような。
そう書いてあるな。なんで触れたんだろ？

### 4.4 制約下での最適化

これはキューンタッカーとかの話っぽいね。
経済学好きでは見慣れた分野なので知らん事も無い気はするが。

slackかどうかをこの本ではactiveかどうか、と呼ぶらしいね。言葉は覚えておこう。

### 4章雑感

これ本当にDeep Learningで使うの？というのが混ざってる気がする。
ニュートン法とか。
本当にその発展の議論があるなら興味深いので楽しみに読んでいきたいが、なんとなく載せただけなら幻滅だなぁ。

soft maxの話とかリプシッツ連続の話が出てくるのはDeep Learnigって感じするね。

Information theoryはそのうちちゃんと勉強します、はい…

# 5章 Machine Learning

目次を見る限り、さすがにこの章は飛ばしてもいいかなぁ、という気もする。

一応各節、見るだけは見てみて、分かってそうな所はバンバン飛ばそう。

### 強化学習もやらんとなぁ

強化学習はこの本のスコープ外だ、との事で、挙げられてる本がちょっと古い。

Sutton and Barato 1998はこれか。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262193981&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

お、comの方はなかなか最近のレビューが多くて好評価な上に、Kindle版があるな。

Bertsekas and Tsitsiklis 1996はこれか。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=1886529108&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

タイトルは全然それっぽくないがcomのレビューを見るとそれっぽいな。ただこちらはレビューの数がぐっと少ない。

DPとの関係とかはちょっと興味がそそられるが、Kindleが無いから読むなら前者か。

次はSuttonのこの本読むかな。
そろそろ強化学習やるか、という気がしてたし。

### 5.2のModel Capacity

あまり知らない言葉として、representational capacityというのが出てきた。
そのあとVC dimensionというのが出てきてるが、これも良く知らない。

AICとかBICの情報基準はPRMLでやって、割と真面目に導出も勉強会でやったが。

あんまり手頃な参考文献も載ってないのと、p111の下の方にあるようにDeep Learningではこの辺の議論は難しそうな印象がある。
結局モデルのcapcity全体を探索してるのとは程遠いのが昨今の現実なので、そっちを議論してもなぁ、という。
実際パラメータの数とかモデルのメモリ的なサイズとか、そういうもっと雑な指標で比較してるよねぇ。そっちの方が便利だったりもするし…

### 5.2.1 no free lunch theorem

なんかこれ、前に菊田さんがこの定理の証明の話して、プリントアウトした物が部屋に転がってるなぁ。
全然読んでないが、この機会に読んでみようかしら？

## NFLの別証明

[No Free Lunch Theoremの別証明と解釈](https://ci.nii.ac.jp/naid/110002812562/)

菊田さんが言ってたのはこれかな。こっちの方が元論文より良いと言ってたのでこっち読むか。

### 2. 定義

2.2の定義で、$$F(d_k)$$が難しいな。
$$d_k$$はxとyの両方の履歴で、Fは評価関数であって、次のxを生成するアルゴリズムは含まないのに注目するとなんだか分かるか。

ようするにその履歴の全$$x_i, y_i$$の組に対して、このマッピングを行う評価関数の集合だな。

で、次の式4はどういう事だろう。

![](https://i.imgur.com/pEMxjYL.jpg)

そのあとに説明があるな。k個のマッピングは既に決まっていて、残り「X全体-k」個のxからYへのマッピングが残るからこうなるのか。なるほど。

そしてaとyの系列を与えると、それに矛盾しないようにfを決められる、という話がある。

aは結局xしか決めないので、
いつもその決められたxに対して後出しで目的のyを用意していく事が出来る訳だ。
これが関数になっている為にはxが同じ所を通っては行けないが、それは前提になっている。

この時点で感覚的には、もうaをどう工夫しようと、全ての評価関数で良い物は存在しない、と言えてしまう気がする。

さて、3章の証明まで行ったら文字が多くなって何が何を表してたのか分からなくなったので、真面目にメモする。

まずaとfが与えられると探索履歴が生成出来る。これを$$S_k(a, f)$$と書く。つまりSは探索履歴。

次に、$$a, d^y_k$$を与えた時に、それに矛盾しないようにfを決めて、その上で作られる探索履歴を$$R_k(a, d^y_k)$$と書く。

### 3. 証明

式9を考えよう。φってのが何か、というのはそんな重要じゃなくて、ようするに右辺のシグマの下の式が左辺のシグマの下の式と同じ物を表している、というのが重要っぽい。

右辺はyの系列ごとにまずfor文を回し、その中で各yの系列に対し、それを実現するfの集合について和を取る。

これはaによらず、全体を漏れなく重複なく舐めるらしい。ふむ。

証明も背理法でそんな難しくないな。

この補題を用いたNFLの証明は、なんか式見るだけだと良く意味が分からないのでサボる。

### 4. 考察

4.1で言ってる事が証明を逆から見てるような形になってて分かりやすい。

ある評価値の履歴があると、どのようなaに対しても、必ず同じ数だけのfがある、というのが証明の内容のようだ。
それは分割の補題と2.2の内容から、感覚的には自然。

ようするに組み合わせの個数（濃度）で議論出来るので、全通りを考えると組み合わせの数は同じっていうのは、感覚的には分かりやすい。

また、この含意から万能分類器を作ろうと努力するのは無意味、というのも分かりやすいな。

なんか理解しやすくてお得な定理だ。

---

さて、本に戻ろう

### 5.4.5 Consistency 

何気なく確率収束が出てる。
こういう、含意を説明せずにいろいろ持ち出すのはこの本の特徴だよな。

分かって無い人には分かって無くてもいい、と思わせるように書かれている。
自分にはそれは誤った幻想に思えるが、実際私とGoodfellowのどちらが正しいかは良く分からない所。
分かってない状態で読んだ人たちが決める事なのだろうな。

### いろいろ読み飛ばす

5.5 最尤推定。この辺の話はさすがに散々PRMLでやったのでいいかなぁ、という印象。

SVMとかdecsion treeもいいだろう。
というかこの辺要るのか？

5.8のUnsupervised learningもembeding的なつながりを意識して書いているかな？と軽く眺めたがそういう空気が感じ取れなかったので読み飛ばす。 
ただ最後のsarseなrepresetationとdistributedなrepresentationの話は真面目に読んでおく。

5.9のstochastic grad descentは文章が長かったので読んでみたが、別段特筆することはない。

5.10の抽象的に書くとMachine Learningのアルゴリズムは一般的に書けるよ、というのも別段新しい事は無し。

### 5.11 DLが欲しくなるような課題

MLの最後に、既存のMLではうまく解けないような課題についてのまとめがある。
ここは真面目に読もう。

curse of dimensionalityに対して、既存の学習手法が織り込むpriorでは不十分で、DLはfactorのcompositionから生成されてる、というpriorを想定する、というのはなかなか興味深い（5.11.2、p155の下部）。

No free lunch theoremを見ると、全てのコスト関数を対等に扱ってはいけない、というのが鍵な訳だよな。

我々は自然界とかに良くあるような画像に興味があるのであって、ランダムのピクセルパターンには興味が無い。
そこで我らが興味ある形を、どうタスクに十分な程度にspecificにしつつ、いろいろな事に使いまわせる程度に一般的に保つのか、というのが我らの目指す事な訳か。

前、Decision treeでrecommend作ってた時に、次元が多いと意外と重要と思ってるフィーチャーの組み合わせに当たってくれない、という事があって、割とcurse of dimentionalityだよなぁ、とか思った事があった。

この時DL的には、ありえそうなフィーチャーのありえそうな組み合わせの関数形は過程しつつ、その組み合わせなどは学習していけるようにネットワークをデザインする、というのがやりたい事なのだろうなぁ。
これに階層性を追加する事でかなり汎用的な物が扱える、というのはありそうな気がする。
次やる機会があったらやってみたいな。

一方でKaggleとかでDT系の方がスコアはいいのだから、なかなか当たらないのを手でフィーチャーエンジニアリングする方が現時点ではいいんだろうなぁ。

### 5.11.3 manifold learning

多様体分かんないお(´・ω・｀)

確率論のあたりの記述を見れば分かるように、こういう自分の知らない分野の記述はきっと学ばなくて良いかのように誤解するように書かれているのだろうなぁ。

やっぱりそろそろ多様体は勉強すべきなのだろうね。

このmanifold hypothesesとかの話はいいな。
word2vecとかから我々はそういう事は考えていて、transfer  learningとかボトルネックをembeddingとして使うのはもちろんそういう考えがあってやっていた訳だが、
それはNFLから考えても王道であり、GANなどもランダムのパターンとは違う、という事を学習させようとする試みと考えられて、いろいろ統一的に理解出来る。

5.11.3は良く書けているな。

## Part 1読み終わり

SVMとかの話とか、Deep Learningでは出てこ無さそうな最適化の話とか結構あった気がするが、あとの方で出てくるのか？
もし出てこないなら、そういうのを出すのはいまいちだなぁ、という印象。

ただ数値計算の所でsoftmaxとかlogのsoftmaxとか良くある話題で話をしているのは好感が持てる。

全体的に対象読者のレベルが良く分からないなぁ、と思うが、不勉強な学部生くらい向けなのかねぇ。
それにしては目次を見てると後半の内容は辛そうだが。
前半と後半で想定読者が違う2つの本がくっついてるのかもしれない。

Deep Learningが解決しようとしている課題についての理解はなかなか良い気がする。
さすがいろいろな事を知ってる人がDLについて書いた内容だな、という気はした。

あと参考文献がそれぞれの分野で定番な物をちゃんと挙げてくれるので、良いスタート地点になりそうで良い。

Part1は5.11以外は読まなくても良いかもしれないが、自分の理解している所、していない所を軽くチェックする目的で見ていくのは悪くない気はした。

# Part 2

Part 2は現状で一番使われてる定番のモデルの要素技術を説明する、という感じらしい。

目次を眺める限り自分の知らない事はあまり無さそうなので、軽く確認する位の気持ちで読んでいこう。

### XORの学習

今更な話題だが、最初からReLUなのは好感が持てるね。

ただこの辺の話はさすがにもういいだろう。

### 6.2.1.1 クロスエントロピーと最尤推定

6.12がクロスエントロピーの式だな、と思って説明を見ると、最尤推定だとの事。
ふむ、と少し考えると、対数尤度の期待値というのはクロスエントロピーなのか。

言われてみると当たり前だが、そういう認識では居なかったな。
KLダイバージェンスの最小化になってる、はPRMLでやった気がするので最大化問題としては同じなのはいいのだが、数式として解釈してなかったというか。

KLダイバージェンスの最小の条件が2つの分布が等しい、という証明をJensenの不等式から出すのをPRMLを読み直して納得したりする。

余談だが、前どこかでこの証明やったはずだが、ググっても自分のブログがひっかからん。ブログにしなかったっけかなぁ。

### 6.2.1.2 変分ベイズの話

なんか最初は分布全体じゃなくて条件付き確率だけでいい場合がある、という話なのに、唐突にそのあと変分ベイズの話を始めてて、つながりが良く分からんな。

なんにせよ、6.14が6.15になると言ってる。
6.14は条件付き確率をfと書けば、条件付き確率のpredictionとラベルデータの二乗誤差の、xとyの同時分布による期待値という式だ。

で、6.15はxの条件付き確率でのyの期待値、と言っているので、xをgivenとした時のyの期待値となっている、と言っている訳だな。

変分ベイズでそんな関係あったっけ？
変分ベイズってqとパラメータを交互に更新してく奴だよな、確か。あんま覚えてないなぁ。
まぁいいか。19.4.2でやるらしいので、そこで真面目に読もう。

6.15は条件付き確率の平均になってるので（分布では無くその統計量）、6.2.1.2の前半の話とつながってるな。なるほど。

ようするにこの節の流れとしては、

1. yの分布の形じゃなくて平均とかmedianで十分な場合がある
2. その場合は変分ベイズが使えて、cross-entropy とは違うロスの変分推定に対応する
3. だが、それはgrad descentと相性が悪いので、平均とかだけが要る場合でもcross entropy使っとけ

という事を言いたいんだな。

6.2.1はコスト関数の節なので、コスト関数の他の形の可能性の話をしている訳か。

## 6.2.2 Output unit

コスト関数の次はOutput unitの話か。
hまでとhからoutputを分けるのは、transfer learningをバリバリやってる身としては自然な分け方だ。

流れとしては

1. 線形
2. sigmoid
3. softmax
4. そのほか

と進むらしい。まぁsoftmaxか線形だよな。

#### 6.2.2.1 線形の場合

6.2.2.1でさらっと最尤推定が二乗誤差の最小化になってる、と書いてあるが、一応確認しておこう。

![](https://i.imgur.com/DIcMiN2.jpg)

雑にやればこんな感じか。

で、これとoutputを線形にする事の関係はなんぞや？

そうか。outputをhと線形の関係で結んだ時、このoutputは何を意味する事が多いかというと、正規分布してるようなyのmeanのpredictionの場合が多い、って事だな。

この時maximum likelihoodはcost関数として二乗誤差を選ぶのに相当する、という話か。

理論的な事を置いとけば、カテゴリカルじゃないなら最後は線形につなぐよな。
で、lossは二乗誤差だろう。
それを確率論的に解釈すると正規分布を仮定して最尤推定している、という話だな。

### 6.2.2.3 softmax

普通にzを線形にするとオーバーパラメとライズだ！と書いてあって、なるほど！と納得した。

でもそのすぐあとで「そうは言っても、そこはあんま問題にならないしオーバーパラメトライズな方が実装はシンプルだ」と書いてあって、俺のなるほど感を返せ！という気持ちになる(´・ω・｀)

### 6.2読み終わり

混合ガウス分布とかはPRMLでさんざんやったので後半はさらさら読める。
自作のoutputレイヤも、log likelihoodからコスト関数作れるぜ、というのは毎回感動するんだが、あんま使う機会無いんだよなぁ。

この辺の話はPRMLの方がずっと分かりやすいね。
ずっと読むのが大変だが。
一度ちゃんとPRMLやったかどうかが、こういう所で違いとなるのだよねぇ。

## 6.3 Hidden unit

ここはDeep Learningっぽい話題なので真面目に読む。
といってもReLUやLeaky ReLUは日常的に使ってるので別段なぁ、という感じ。

唐突なmaxout推し！へー、と思いながら読む。

こういう個々の要素の理解は全体をいじってる時に効いてくるかもしれんので、読む気にはなるね。

学習の為にlinearに近づけるというアイデアとしてLSTMも解釈出来るというのは興味深いな。なるほど。10.10でその辺の話をするとの事なので楽しみだな。

そのほかのhidden unitと何故最近使われないのか、の話も勉強になった。
6.3はなかなか良く書けてる節だね。

## 6.4.1 層の数の話

Universal approximation theoremの話から、ではなんで層が多い方がいいのか？という話。
これもなかなか良く書けていて勉強になる。

現在分かってる事、分かってない事、感覚的な解釈とバランス良くて、これを書いた人はDeep Learning良く分かってんなー、って感じがする。当たり前だが。

### Back propagationはさすがに飛ばす

ざっと見ていったが、読むべきものは無さそうなので飛ばした。
Hessianのあたりとか歴史とかは読んだが、あまり得るものは無かった。

# 7章 Reguralization

さすがにこの章は要らないかなぁ、と眺めていくと、意外と読み応えある所あるので、面白そうな所は読んでいく事にする。

### 7.1.1 L2 regularization

このregularizationの効果を調べる所がいろいろ計算してるので読む。
PRMLでpriorに関連している、というのはやった訳だが。

7.11式あたりで固有値分解をして解釈をいろいろしている。
やはりDeep Learningで必要な線形代数といったらこの辺は入るよな。
[機械学習をやる上で線形代数のどのような知識が必要になるのか](https://yoheikikuta.github.io/linear_algebra/)の話だが、って今見たら追記があるな。やはりこの位は要るやね。

で、なんか全然Deep Learning特有の話が始まらない。
章構成的にはここはDL特有の話を扱う章のはずだが…

### 7.1.2 L1 Regularization

こちらの議論はDeep Learningっぽさを感じるな。
feature selectionに使える、との話。

あんまり自分は使った事無いが、モデル圧縮とかやってる人達はこの辺の理屈を使うのかしら？

### 7.2 制約条件と考える場合

パラメータに有る種の制約条件を課すのは、キューンタッカー定理を適用する話となる。
これはイマドキなトピックだね。

パラメータに特定の範囲、という指定を加える事を関数空間をリプシッツ定数1以下に制約する事の近似とみなしたりするのはW-GANでやってる訳で。

理論的な話はあまり詳しくやってくれてないが、reprojectionという手段もある、というのは、へー、という感じ。
使った事無いが確かにこっちの方が良い可能性はある。
でもtensorflowでどうやって実装するのかしらね？

### 7.5.1 ouptutのターゲットにノイズを加える

このaugmentationって使った事無い気がするな。
最後のsoftmaxのターゲットを変えるだけで簡単そうだし、確かに効果ありそうな気がする。

### 7.8 Early stoppingの話

L2 regulrizationに一致する、という話はどこかで見た事ある気がするが、
頭の中で証明の筋が思いつかなかったので真面目に読む。

極限との差分の漸化式が7.37になるので、0からタウまで遷移させると7.40となり、タウを含んだ式で表せる。

あとはregularizationの極限と比較するお間の行列の値が同じになるようにパラメータが選べる、と。

さらなる近似でタウはL2 regularizerのパラメータと7.44の関係式で結ばれる。
感覚的には早く止める事はpriorから離れすぎない所に引き止めるのだから、こんな関係式があっても不思議じゃないやね。

### 7.10 Sparse representations

OMP-kって知らないな。
我らがNg先生がOMP-1が一番良いとおっしゃってるが。

気分的にはPCAみたいな物に見えるが、Wをどう学習するのかは良く分からん。
元論文読め、という事なんだろうな。

### 7.12 dropout

dropout boostingって知らないな。
この本の説明だけては良く分からんが。

ただ最近そんなにオーバーフィッティングしないんだよなぁ。
みんなtransfer learningでそんなオーバーフィッティングさせられてるの？
それって相当良いトレーニングセットが用意出来てるという事だと思うが、
そういう実サービスってそんな多くないと思うのだけど。

中間レイヤーが消える事の意味、みたいなのの話はなかなか面白い。
dropoutってそういう物だとなんとなくは思っていたが、ちゃんと文章にして解釈をまとめる事には意味があるな。

### 7.14 tangent propとか

tangent prop自体はPRMLの方が詳しいし、別段優れた説明とも思わないが、
adversarial exampleをヘッシアンをpertubationに強くするreguralization としてそれぞれの関係を理解するのは、そうか、こういう研究の延長でGANが出てきたのか、と思う。 

この辺のregularizationの話で、自身の業績をどういうコンテキストに置いて説明しているか、というのが読めるのはいいね。
Goodfellow御大のニューラルネットに対する理解みたいなのが見えてくる気がする。

そして関連研究も当然詳しいが自身もたくさん論文書いてる、というのは、当たり前だがトップ研究者とはこういうものだ、というのを見せつけられるな。
他人の論文RTするだけで自身の研究が無い、肩書だけ偉い人達とは一線を画す。
こうなりたいものやね。

トップカンファレンスに通すのはこのままでは難しいな、とか思ってる自分からすれば雲の上の話だが。

などとポエムとして読んでいたら、autoencoderがtangentを学習してる、
という話が出てきて姿勢を正して読み直す。
ふむ、興味深いな。14章が楽しみだ。

# 8章 Optimization

gradient descentの話をしたのだがらOptimizationなんてadamとかmomentumとかの話くらいしか無いのでは？と思うが、なんか大きく扱われてる。
へー。

### 8.1.3 バッチサイズの話

p272で、ミニバッチの典型的なサイズは32から256で、16が良く使われる、と書いてある。
へー、ミニバッチってワープを使い切れるように多めに指定しておくのが良いと思ってたが、こんなもんなのか。

一度のコンボリューションが16x16とかだとすれば、depthは4くらいあればまぁまぁなのか。
確かに16あれば2018年現在のGPUならだいたい埋まるかね。

[V100のSMの数は80個か](https://devblogs.nvidia.com/inside-volta/)。
で、1SMが最大64warp、スレッド的には1SMは2048か。
あれ？埋まらなくない？

まぁ前のmini batchの別のレイヤー計算したりするから別にこれでいいのかな。

mini batchの方が収束が早いというのは聞いていたが、こんな小さい方がいいんだ。知らなんだ。

そのあとにgradベースなら100とか小さなバッチサイズで良くてヘッシアン使うなら1万くらい要る、とか言ってる。
あれ？100で小さいって、さっき16くらいでいいって言ってなかった？なんかいい加減だな。

まぁこの手のはInception v3の原典とか、transfer learningの原典とか、使うモデルや手法の原典に従う方が良い気はする。

### 8.2.3 saddle pointとかの話

8.2.2の局所最適もそうだが、saddle pointが問題になる事ってあるのかしら？
全部のミニバッチがsaddle pointになるケースなんて実用上無いと思うのだが。

この項ではDauphineって人が良く登場するな。初耳の人名だがとんな人かしら？

### 8.2.7 局所的な性質だけの学習の限界

そのまま読んでいくと、8.2.7でそもそも極値にたどり着かない、という話をしてる。
そうそう、だから8.2.3の話とかはあんま重要じゃないと思うのだが。

8.2.7の最後のところを読む限り、

### 8.3.1 learning rateの指定

突然アドバイスが具体的に（笑）

そうそう、そんな感じで指定するよね、と思うが、これまでの細かい理論の話はなんだったんだ、という気もしてしまう。

### 8.3.2 Momentum

最近はいろいろ特殊なモデルをトレーニングする時はSGDが一番わかり易い振る舞いで使いやすい、というのが持論なので、momentumとかいまいち信用してないが、
いろいろ書いてあるのでちゃんと読む。

adamもそうだが、この辺の話はその当時のネットワークに最適化しすぎてると思うんだよなぁ。

## 8章はなかなか進まないなぁ

細かいところで割と先進的な話題を混ぜてくるので、8章はなかなかサラサラ読めない。
特に未解決な話はあまり知らないので、へー、そんなのがactive research areaなのか、と思いながら読むと時間が掛かる。

一方で掛けてる時間だけ何かを学んでるか？と言われると微妙。
難しい訳じゃないし読んでて面白いが、
これを読んだ結果を何かに活かせるかなぁ。

### 8.5.3 Adam

ここまでの流れからするともっと詳細な話があると思って読んでたら擬似コードだけで解釈の話があまり無くて驚いた。
著者の好みが出てるのかね。
