---
title: "GoodfellowのDeepLearning本を読む"
date: 2018-05-22 01:44:27
---

プログラム言語の本もだいたい読んだので、次は機械学習の本でも読むかな、と思い、Goodfellow本を読む事にする。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262035618&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

ちょうど年末に買っておいたので。

### どう読むか

この本は多分Deep Learningをバリバリやってる人向けでは無く、これからやっていきたいという人向けにいろいろ紹介していく、という物と自分は思ってる。
Deep Learningという物を良く知らない人が基礎から理解していき、最新の研究の入り口くらいまでの話題をいろいろ知る、という本なんじゃないか。
（まだ読んでないので予想だけど）

一方で自分は画像認識に関してはプロフェッショナルといえる水準だと思う。
実サービスで使っていく難しさとか自分たちなりの解決など、十分な経験がある。
だからこの本が目的としている、「読者をここまで連れていきたい」という場所よりは、既に先に居るんじゃないか。

この本が出たあとの論文もいろいろ読んでいるし、モデルもいろいろ触っている。だから自分にとっては、いろいろなトピックや研究を知る、という目的でこの本を読むのは、いまさらな感じがする。
変化が早い分野だからねぇ…。

目次を読む限り、トビックとしてはPart 3の知らない所だけ読めば事足りそうだが、それではたぶんあんまり読む事は無い。それが一番効率的な読み方かもしれないけど、どうせ暇なので、もうちょっと違う所を読んでみたいな。

ということで、せっかくGoogfellow御大が本を書いてくれたのだから、彼がニューラルネットをどう捉えているのか、みたいな、著者個人の感性みたいなのに注目して読んで行こうかなぁ、と思う。うまく行くかは分からんが。

逆に読者がDeep Learningを学ぶ為に必要、という感じの記述はガンガン飛ばして行きたい。
例えば今更Drop outとかBatch normarizationなどの話は読まなくて良かろう。

# Introduction

自分の目的を思うと、このintroductionとかのポエムパートの方がむしろ重要かもしれない。

MLPがrepresentation learningのような物だ、というのはいいとして、
マルチステップのプログラムを学習しているような物と見れる、というのは面白い。
確かに中間レイヤのWが何に使われているのか、というのは、多様な見方を身に着けておく方が良い気はする。

### 想定読者

どちらも自分は入って無さそう。
その辺はそんな気はしてたので、自分に必要なように読んでいきたい。

### history

brainの模倣の路線では無くなった理由、とかは興味深いな。

wax and wane 盛衰。月の満ち欠け。waxに月が満ちるって意味があるのね。

### Introduction読み終わり

これが書かれた時とその後のトレンドは結構違うな、と思う。

ネットワークは最近はむしろ小さくなっているし、タスクが汎用になっているよりはより難しいタスクに挑む傾向がある気がする。

ある時点で振り返った時に続いている傾向がその後も続く訳では無い、というのは、重要な洞察だよな。

ただある一時点で過去からの流れをこうしてまとめて見る事は、そういった一歩引いた視点で物を考えるのには有用な気がする。

なかなか良く書けてて面白い導入だった。

# Part 1 Applied MathとMLの基礎

ここはタイトルからすると飛ばし読みでいいかなぁ、という気がするが、軽く見てから考えよう。

## 2章、線形代数

あつかってるトピックを眺めていく

- 行列、テンソルの定義
- 行列積、Identity、Inverse
-  Linear dependenceとspan

spanとはなんぞや？という事で2.4は軽く見てみる。
基底とか線形結合の話か。
この辺は日本語で勉強したので英単語知らないから、単語の勉強として軽く読む。
linear dependenceは線形従属の事やね。

そのあと特異値分解とかpinvが雑に紹介されてる。この辺をラプラス展開からちゃんとやらないのは気に食わないが、そういう本だという事だ。

reciprocalは逆数。良く出てくるがすぐ忘れる単語。

Frobeeius norm がトレースで表せる、という式2.49、見た覚えあるが、成り立ってるか軽く試す。

![](https://i.imgur.com/x3pDBXr.png)

成り立ちそうやね。

### 2章読み終わり

線形代数の提示の仕方としてはあまり好きになれない内容だが、線形代数の本じゃないのでこの辺はどうでも良い。

## 3章 確率論と情報理論

確率論は割と本気でやった領域なのでこの本で何か知る事があるとも思えない、という事で軽く流す。

最初に確率論ちゃんと勉強するならJaynes 2003を読め、と書いてある。

これか。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0521592712&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

amazon.com側ではレビューがいっぱいついてて、ベイズ統計の教科書っぽい。
jpでは一件もレビューがついてないのは、きっとみんなGoodfellowが提示してるリファレンスなんて見ても居ないからに違いない。

目次が読みたかったのでKindleにサンプル送ろうとしたら、おまえの国には送らん、と言われる。ぐぬぬ。

[本家のサイト](http://www.cambridge.org/gb/academic/subjects/physics/theoretical-physics-and-mathematical-physics/probability-theory-logic-science?format=HB&isbn=9780521592710#VQP13Vzbv7tYKUI5.97)はGoogle previewというので目次が見えるな。

ベイズ統計メインでタイトルのように論理的な推論の拡張として考える話があるが、それ以外のトピックもかなり多様に見える。

decision theoryとか結構長い。
毎回ちらっといろんな本に出てくるが何を言いたいのか良く分かってないので一度やってみても良いかもしらんが、まぁ今はいい。

数学的には初等的な範囲に見える。
あくまでベイジアンな応用をちゃんと扱う、という感じで、解析的な扱いはあまりして無さそう。

なかなか本格的な本ではあるが、さすがに生成モデル全盛の今の時代にこのレベルでは辛いのではなかろうか。

### 確率変数のあたり

3.1でfrequentistとbeyesian論争的なポエムがしばらく書かれたあとに、3.2で確率変数が出てくる。

「A random variable is a variable that can take on different values randomly.」
 
えっ？まじ！？

ってこの説明ではさすがにディープラーニングの話は出来なく無いか？
でも一パラグラフで確率変数の話を終えて次の3.3に進んでしまっている。

Goodfellow先生は確率論の解析的な話とかは全然重視てしないのかなぁ、数学なんか詳しくなくてもepoc makingな論文は書ける！という事なのだろうか。
さすがGoodfellow先生ほどの実績を残す男は違うな…  
ふむむ、考えさせられる。

と思い先を読むと、3.3.1でProbability Mass Functionという名前で結構突っ込んだ分布の確率測度の話が、そうした学術用語を出さずに続く。

えー、そりゃ無いよ！Goodfellow先生！

これはこんな書き方では測度論知らない人は何を言ってるのかすら理解出来ないだろう…

random variabieのstateとか完全に未定義な単語でいろいろ語るが、どう見てもボレル集合族の元の話だし、PMFが可測関数だって話だよなぁ、これ。

うぉ、そのあとに確率測度の満たす性質でもっとあらわに書き始めた！酷い開き直り！

うーん、ここまで開き直るくらいなら、最初からボレル集合族という言葉を出す方がずっと誠実だと思うんだが…

追記: 3.12に測度論の話あり。そちらに3.3の感想を追加。

### 初歩的な確率の関係

3.4から3.7まででマージナライズや積の法則や条件つき確率などの基本的な話を短くしてある。

この辺は確率論を忘れてる人が思い出す為、という感じで、初めて学ぶ人が読む物では無いし、この辺普段から使ってる人が読む物でも無い。

### 3.8 期待値とかcovとか

期待値はGANとかではかなり込み入った話となるので、注意して読もう、と思ったが、これがまた短い。
ただ普通の入門書の期待値よりは記法の話が細かい。これは必要だからだよな。

covの定義などの書き方も、統計の入門書よりは、より論文的になっている。

そのあと独立とcovがゼロの関係とかの話がダラダラとされている。この辺の突然素人向けの注意が挟まれるのはDeep Learningという言葉に寄ってくる人の性質を表しているなぁ。

### 3.9 良く出てくる分布

ベルヌーイ分布やガウス分布はおなじみだが、ポワソン分布やガンマ分布が出てこないのがディープラーニングやね。

やはりガウス分布の扱いは長い。

そして指数分布やらプラス分布、ディラック分布などはなかなからしさがある。
emprical distributionも機械学習らしさがあるね。

混合分布も意外と真面目に扱ってる。Gaussian mixtureとかやるの？
16.5で構造推定みたいなのやるらしい。
なんかPGMっぽいなぁ。

### 3.10 sigoidとsoft plusの性質

soft plusってあんまり知らないのでこの機会に覚えておく。
ついでにシグモイド関数の微分とかの式を眺めておく。

### 3.12には測度論の話がある

3.12まで読むと、先程まじかよ、って思った3.3.2は真面目にやると測度論が要る、という話をする。

だが3.3.2のどの辺が測度論が分かってないと良く分からないのか、という話はせず、機械学習の対象では病的な奴は出てこない、という話がある。
また、むしろ証明などを記述する道具として便利だ、と述べている。
この見解は自分と同じなので納得は出来る所。

でも3.3.2の話を測度論がわかる人向けにしないっていうのはどうなんだろう？これでは測度論を知らない読者が、その必要性を知る事は出来ない。

### 確率論の話を読んでの雑感

自分は実解析を最近かなり真面目にやってたのでその周辺の話を。

この本は、測度論の話を出す時に、測度論の用語を使わずに書く。
その結果、おそらく測度論を知らない人は、たまに良く分からない記述が混ざるが、別段分からない事は無い、と感じるんじゃないか。

でもこのたまに良く分からない所が混ざる、という所が後ろの方ではたぶん分からない元になるはずと思う。

少なくとも自分はかつて、後ろの方のトピックの元論文を読んだ時に良く分からないからどんどん遡っていった結果実解析をやり、その結果これまで分からなかった事がいろいろ分かるようになった。
だからまだの人はきっと私とは逆向きに同じ過程をたどるんじゃないか、と思う。

ただ、この本ではその分からない理由が意図的にわかりにくく書かれているように見える。
それは何故だろうか？

まず一番ポジティブな解釈としては、本当にこの辺の事を知らない人でも、だいたいは理解出来るように書ける、と思って書いた可能性。
私は3章の終わり時点までで既にそれは失敗していると思うが、
本当にそれを目指して書いているなら、応援したい気持ちはある。
たぶん誰かがそれをやれば大分関連分野の数学的敷居は大きく下がる、と自分も長らく思っていた所なので。

一番ネガティブな解釈は、どうせDeep Learningとかいって騒いでる連中のほとんどは内容が理解出来る訳では無いので、そういう連中は間違って買ってしまえばよろしい、と思ってそういう本に仕立てた、という事。
さすがにこれは無いか。

現実的には、これで分かると思って書いているが全然分かんねーよ、ってあたりな可能性はある。
Goodfellowが確率の根本原理に詳しすぎる結果、3.3の書き方で自然とボレル集合族的な物が感覚的に分かってしまうので、皆も分かるだろう、と思ってるとか。
これはありそうだなぁ…

さて、著者の意図はおいといて、読者の我々はどう向き合うべきか、という事を考えよう。

ここまで読んだ限り、必要な事に関しては、分かっている人が読めば分かるが、分からない人が読むと良く分からないように、という形で、一通り書いてあるように見える。

だから自分が分かっている分野に関しては、「これとこれとこれを分かってればいいのね、おっけー」と思って読めば良さそう。

問題は自分の良く知らない分野だ。
自分の良く知らない分野で、何を言ってるのか良く分からないが、なんかどうでも良さそうな事を言ってるのを見つけた時。

これはたぶん分かる人向けのメッセージなので、たぶん分かる方が良い。
特にあとの方で関連しそうな所でさっぱり分からなくなった時は、ここに立ち戻って、その関連分野を真面目に学ぶ必要があるのだろう。

だから、どうでも良さそうだが何を言ってるか分からない物を見つけたら要注意だ。ブックマークなりこのブログにメモなり残して、あとで分からない所が出てきた時にそこ由来かチェックしよう。

後半の内容読んでみないとここまでの印象があってるかは分からんので、読み進めてまた感想書く。

### 3.13 Information Theory

この節は以前職場にこの本が置いてあって、立ち読みした事がある。

自分はこの分野はやった事無い。
ガラケー屋だった時にCDMAなどでこの辺の話をちょっとやったのと、あとはKLダイバージェンスやエントロピーを使ってる、くらい。

PRMLでもこの本でも情報理論に言及があるので、真面目にやった方が良いのかもしれない。

最初に紹介されている本はCover and Thomas (2006)とMacKay (2003)らしい。

Cover Thomas 2006はこれで、

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0471241954&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

MacKey 2003はこれか。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0521642981&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

MacKeyの方が自分には合ってそうだが、本家のサイトにも電子版が無い…  
うーん、スキャンするのかったるいなぁ。どうしたもんか。

さて、内容に移る。
エントロピーがその分布をエンコードする下限のビット数の期待値となる、というのは自分は理解してない所で、
どうでも良く見えるがこれは知ってる人向けのメッセージなのだろうな。

KLダイバージェンスの追加に必要なうんたら、というくだりも良く分かってない。

KLダイバージェンスと非対称性の図3.6は、まったく同じものをPRMLで見た気がする。

良く忘れるのでKLダイバージェンスの式は書いておくか。p72。

![](https://i.imgur.com/sdTkGHm.jpg)

クロスエントロピーとかの話はPRMLのEM法で結構やったので理解出来てると思う。

### 3.14 Structured Probabilistic Models

この辺は超得意分野だぜ〜

- [PGMコースの受講メモ](http://jbbs.shitaraba.net/bbs/read.cgi/study/12706/1466006216/)
- [PRML勉強会、36〜60](Pattern Recognition and Machine Learning
http://jbbs.shitaraba.net/bbs/read.cgi/study/12706/1481675604/36-60)

以前junction treeアルゴリズムとかをちゃんと理解する為に、以下の本と、

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4320111397&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

あとDaphneの本も買って、面白そうな所だけつまみ読みした。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262013193&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

内容としてもベイジアンネットとマルコフネットの話で何も分からん事は無い、という感じ。
Part3で突っ込んだ話をする、との事で楽しみだ。

# 4章、数値計算

ペラペラめくった所、grad descentとかキューンタッカーの定理とかの話っぽい。
飛ばすか悩むが、軽く読んでおくか。

### 4.3.1 JacobianとHessian行列

4.3のgrad descentの説明まではさらさらと読んで行った。で、4.3.1のHessian。

HessianはPRMLで出てきた時に、結局何に使うのか解説されてなくて良くわからんなー、と思ってた所なので真面目に読んで見る。

Hessianが対称とかgradのヤコビアンだ、というあたりまではいい。

そのあと、単位ベクトルdによるsecond derivativeが $$d^TH\ d$$で表せる、みたいな話は知らない話だな。

軽く手を動かしておこう。
まず1階の微分から。p82に載ってるが、この手のは久しぶりにやる時は添字をサボらないに尽きる。

簡単の為、R2からRへの射影の場合をやろう。

![](https://i.imgur.com/mZW5olY.jpg)

よし、だいたい思い出した。
では二階の微分をやってみよう。

![](https://i.imgur.com/bevYxhn.jpg)

![](https://i.imgur.com/GkorLu1.jpg)

![](https://i.imgur.com/YXzmnvW.jpg)

そのあとdがヘッシアンの固有ベクトルなら二階の微分が固有値になる、というのは、簡単に出せるな。

![](https://i.imgur.com/F8fwr7t.jpg)

固有ベクトルじゃない時は、固有値の重み付け和になる、という話だが、これは対象とする方向ベクトルを固有ベクトルの線形結合で書けばそうなるだろう。

で、この方向ベクトルを変えていって二階の微分最大の場所は固有値最大の固有ベクトルの方向になる。
これもほぼ明らかだね。

さて、これを使ってのstep sizeの決定。
4.10は4.9をイプシロンについて平方完成すれば出そう。

そしてgradを先程の固有ベクトルの線形結合で表す議論を思い出せば、ラムダmaxの逆数になるのも明らか。

次にこのHessianの使い方として、ヘッシアンの固有値が大きく違う方向があると、片方で適切なラーニングレートではもう片方にとって適切でなく、いったりきたりを繰り返して無駄に進むケースの図がある。

そして二次の項まで考えたニュートン法の式がある。

この辺の話は使うのかね？ 
そのあとに鞍点ではgrad descentに劣る、とか書いてあるな。やっぱり使わなくない？

### リプシッツ定数の説明が！

そのあとに唐突にリプシッツ連続とリプシッツ定数の説明が。
この辺に触れてるのはDeep Learningっぽいね！後半どう使うのか期待！

そのあとにはconvexなケースの話があるが、Deep Learningには関係ないような。
そう書いてあるな。なんで触れたんだろ？

### 4.4 制約下での最適化

これはキューンタッカーとかの話っぽいね。
経済学好きでは見慣れた分野なので知らん事も無い気はするが。

slackかどうかをこの本ではactiveかどうか、と呼ぶらしいね。言葉は覚えておこう。

### 4章雑感

これ本当にDeep Learningで使うの？というのが混ざってる気がする。
ニュートン法とか。
本当にその発展の議論があるなら興味深いので楽しみに読んでいきたいが、なんとなく載せただけなら幻滅だなぁ。

soft maxの話とかリプシッツ連続の話が出てくるのはDeep Learnigって感じするね。

Information theoryはそのうちちゃんと勉強します、はい…

# 5章 Machine Learning

目次を見る限り、さすがにこの章は飛ばしてもいいかなぁ、という気もする。

一応各節、見るだけは見てみて、分かってそうな所はバンバン飛ばそう。

### 強化学習もやらんとなぁ

強化学習はこの本のスコープ外だ、との事で、挙げられてる本がちょっと古い。

Sutton and Barato 1998はこれか。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262193981&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

お、comの方はなかなか最近のレビューが多くて好評価な上に、Kindle版があるな。

Bertsekas and Tsitsiklis 1996はこれか。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=1886529108&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

タイトルは全然それっぽくないがcomのレビューを見るとそれっぽいな。ただこちらはレビューの数がぐっと少ない。

DPとの関係とかはちょっと興味がそそられるが、Kindleが無いから読むなら前者か。

次はSuttonのこの本読むかな。
そろそろ強化学習やるか、という気がしてたし。

### 5.2のModel Capacity

あまり知らない言葉として、representational capacityというのが出てきた。
そのあとVC dimensionというのが出てきてるが、これも良く知らない。

AICとかBICの情報基準はPRMLでやって、割と真面目に導出も勉強会でやったが。

あんまり手頃な参考文献も載ってないのと、p111の下の方にあるようにDeep Learningではこの辺の議論は難しそうな印象がある。
結局モデルのcapcity全体を探索してるのとは程遠いのが昨今の現実なので、そっちを議論してもなぁ、という。
実際パラメータの数とかモデルのメモリ的なサイズとか、そういうもっと雑な指標で比較してるよねぇ。そっちの方が便利だったりもするし…

### 5.2.1 no free lunch theorem

なんかこれ、前に菊田さんがこの定理の証明の話して、プリントアウトした物が部屋に転がってるなぁ。
全然読んでないが、この機会に読んでみようかしら？
