---
title: 系列データを扱うモデルは何が良いのか？
layout: page
---

現状、ストロークデータからシンボルと座標とストロークの終わりを判定するモデルを作って、
それで前処理したいな、と思っている。([てがしきのモデルを考える]({% post_url 2019-06-04-103045 %}))

終わりを判定するのはちょっと見かけないタスクだが、ある種のregressionとして、
TCNの最後の出力の一つの次元を長さとして、これを誤差の二乗をロスとして足せばよいかな、とか思っている。
全体的にはSequential MNISTに似たタスクとなる。

上ではTCNと言ってあるが、ようするに系列データのclassificationの最強のモデルを使えば良い。
TCNの延長でConvS2Sの論文を読んでたら、これならもうTransformerでもいいんじゃないか？という気がしてくる。
そもそも系列データって何で扱うのが良いのだろう？
dialated ConvとRNNの比較はTCNの論文でまぁまぁ分かるのだが、transformerのNLP以外への応用ってどうなのか？そもそも系列データのclassificationは今何を使うのがスタンダードなのだろう？
この辺について、自分の理解と調べた事をメモしてみたい。

なお、あまり整理して書いている訳じゃないので、パラグラフ分けには大した意味は無い。

### RNNとdilated Convolution

系列データというと、一昔前はRNN系列が多かった。
特にseq2seqの、いわゆるmany to manyの学習にはencoder-decoderが唯一実用になったアーキテクチャで、これが良く使われていた。
その影響もあって、encoder-decoderじゃない系列データでもとりあえずRNN系列を使う、というのが基本になっていた。

でも最近はWave Netが音声合成で凄い良い結果を出した影響から、dilated Convolutionの方が良いんじゃない
か？という話が出てきて、TCNでは割としっかりとした比較が為されていて、しかもTCNは割と汎用で使いやすいので、系列データにはCNNの方がいいんじゃないか？という流れがある。

- [arXiv:1609.03499 WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499) dilated convolutionがはやった元論文
- [arXiv:1803.01271 An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271) こちらがTCN。音声合成以外でのdilated convolutionの評価と使いやすいモデル

RNN系列も細かなチューンがいろいろあって、どちらがスコアが良いか、というのは一概には言いづらい。
例えばdilatedなRNNを推す論文、[arXiv:1710.02224 Dilated Recurrent Neural Networks](https://arxiv.org/abs/1710.02224)では、パラメータの都合でdilated CNNの方が少しパラメータが多くなってしまうのだが、その比較でもsequential MNISTなどではdilated RNNの方が良いと言っている（表を見ると、だいたい同じ、が結論に思うが…）。

ただ、LTSMはバッチで学習させる時にhidden stateを全部GPUメモリにとるので長い系列の学習がTPUとかではやりにくい。
アーキテクチャとして系列データを自然に扱えるので最初の選択肢になりやすいが、言う程長い系列は扱えないし、短い系列に決め打つならCNN系列の方がシンプルな学習になる気もしている。（業界的にもそういうトレンドはある気がする）。

自分が調べている感じ、classificationならRNN系列よりはTCNの方が学習が容易な気がしている（が、自分が作ったTCNはいまいちうまく動いてない。これは実装にバグがあるんじゃないか？と思ってる）。

TCNの論文ではsequential MNISTなどの比較がある。

### ConvS2Sとtransformer

encoder-decoderでも、Conv S2Sやその後継はCNNベースのencoder-decoderで、これもRNNベースのseq2seq系列に比べて劣る物では無さそう。

- [arXiv:1705.03122 Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)

Conv S2Sはpositional encoding入れてconvolutionをかけた物にアテンションを入れて結果を出す。
構造的には結構Transformerに似ている（というかTransformerがこの辺の発展形なのだろう）。

### Sequential MNISTの比較いろいろ

Transformerは自然言語処理の話が多くて、あまりSequential MNISTなどの系列データの処理の話は出てこない。
ただ、原理的にはTCNで良いならこれでも良さそうな気はする。

sequential MNISTでのtransformerと工夫したRNNの比較は、[arXiv:1803.00144 Learning Longer-term Dependencies in RNNs with Auxiliary Losses](https://arxiv.org/abs/1803.00144)にある。
transformerの最後をaverage poolingした後にFCにつなげている。
この論文では長い系列の学習を売りにしているが、2kくらいのシーケンスの長さではTransformerはめちゃくちゃ良い。

[arXiv:1810.06682 Trellis Networks for Sequence Modeling](https://arxiv.org/abs/1810.06682)でも上の論文の引用も含めて、いくつかの最近のモデルのsequential MNISTなどの比較が載っている。
dilated RNN, transformer, TCNはほぼ同じくらいのパフォーマンス、という結論に見える。
ただTransformerでいいんじゃないか？という気はしてくる結果。

だいたいは比較対象といて登場するTCNとtransformerだが、どちらも悪くない結果に見える。

### 結論

系列データを扱う場合、dialted CNN（TCN）とTransformerは結構良さそうだが、dilated RNNなどの工夫の入ったRNNもだいたい良い。

TransformerとTCNを自分はもっと使っていった方が良い気がする。