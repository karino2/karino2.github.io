---
title: "Elements of Information Theoryの十二章〜"
layout: page	
---

[Elements of Information Theoryの読書記録](https://karino2.github.io/2019/02/10/143600.html)

- [2章までのノート](https://karino2.github.io/2019/01/31/115955.html)
- [3章〜8章のノート](https://karino2.github.io/2019/02/10/144121.html)
- [9章〜11章のノート](https://karino2.github.io/2019/03/07/114649.html)

という事で以下の本の12章から先を読んでいきます。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=B00HLG9ISQ&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

# 12章、Maximum Entropy

そもそもこの本を読もうと決意したきっかけがMEMMとかCRFでMEが使われてて、自分が全然知らなかった事だから、というのがある。
なのでなかなかモチベーション高い。

### 変分計算の公式

式12.2から12.3の変形みたいなのを、どう計算するか。

要するに一つ目の項だけ見ると以下。

![images/2019-03-22-135325/0000.png]({{"/assets/images/2019-03-22-135325/0000.png" | absolute_url}})

最終的に極値を取る関数がオイラー方程式で求まるのは良いとして、これの変分自体を計算する簡単な公式があるんじゃないか、と思ってた。

自分は良く分からないので、手持ちの変分の教科書に従い、定義から計算してた。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=B00JKIJKAE&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

なおこの本は大変分かりやすく書かれてて、最初の10ページくらいで必要な事は全部わかる。そこだけpdfかなんかで配ってればそれで十分なんだが…ただそんな高い物でも無いので、そこだけの為に買う価値はある。

で、これに従い、1.1の公式6から計算する。

![images/2019-03-22-135325/0001.jpg]({{"/assets/images/2019-03-22-135325/0001.jpg" | absolute_url}})

これなら隅から隅までなにをやっているかを理解出来ているのだけど、これを式12.2の各項でやってるとも思えない。

もっと簡単な方法があるんじゃないか？とslackで物理な人に聞いた所、以下のような公式で計算出来る、と教えてもらった。

![images/2019-03-22-135325/0002.png]({{"/assets/images/2019-03-22-135325/0002.png" | absolute_url}})

右辺のデルタはディラックのデルタ関数。
これで積分記号が消える。

これに従い式12.2を計算すると、

![images/2019-03-22-135325/0003.jpg]({{"/assets/images/2019-03-22-135325/0003.jpg" | absolute_url}})

おお、これなら慣れれば暗算で出来そう。
これって昔どっかで見た記憶があるが、10年以上前な気がするのでたぶん学生の頃だよなぁ。全然覚えてない。

PRMLのAppendixを見ても上記の変分の本を見ても書いてないので、まぁまぁ知らない人もいるんじゃないか？これ。

というのは、結果だけ丸暗記ならオイラー方程式で良い。オイラー方程式の導出自体は定義に戻ってやれば出来る。
一方でたまにしか出てこないなら定義に戻って計算すれば良い。

ということで、この、間くらいの変分のルールって結構ちゃんと勉強してないと必要にならないから、あんまり解説される機会が多くない気がする。
そしてすごく使う人にとっては常識になってしまうのでますます語られない。

上記の公式は、本質的には普段自分がやってた計算をデルタ関数で記述しているだけなので、教えてもらえば、それを理解するのはそんなに難しい事では無い。一方でこれを知らないと計算は大変。

という事で変分レベルが1上がった（ファンファーレの音）

追記: この辺の計算が載ってる教科書とか知ってますか？と言ったら、菊田さんがググって以下を見つけてくる。
[Density Functional Theory : An Advanced Course](https://cds.cern.ch/record/1383342)のページの[Back matterというappendix](https://cds.cern.ch/record/1383342/files/978-3-642-14090-7_BookBackMatter.pdf)(ページの下にリンクがある)

これのAppendix Aは必要な事がすべて書かれている気がする。
ただちょっと知ってる人向け、って感じですね。前述の本を読んでからだとちょうど良いですね。自分にはちょうど良さそう（なお教科書はめちゃくちゃ難しそうで何についての本なのかも分からん…）

### Approach2の計算を追う

大した事無いが一応追っておく。

![images/2019-03-22-135325/0004.jpg]({{"/assets/images/2019-03-22-135325/0004.jpg" | absolute_url}})

log fはfで期待値をとってもgで期待値をとっても値が一緒、というのがポイントか。

次の定理12.1.1でも同じ性質を中心に証明されてる。

ところで、- g ln kの形の期待値を最小にするdensity kは、gなんだろうか？
もしそうなら、積分記号をさぼると、以下みたいなかんたんな式変形と解釈出来るが。

- g ln g ＜ - g ln f = - f ln f

少し考えてみよう。

![images/2019-03-22-135325/0005.jpg]({{"/assets/images/2019-03-22-135325/0005.jpg" | absolute_url}})

お、言えそうだ。こういうのスラスラ解けるのは変分レベル上がった感じするね。

でもこれは、ようするに12.6と12.7と同じ事か。
そのやり方も書いてみるか。

![images/2019-03-22-135325/0006.jpg]({{"/assets/images/2019-03-22-135325/0006.jpg" | absolute_url}})

お、absolutely continuousじゃないとKLダイバージェンスが定義できなさそうだな。
こんな所で必要だったのか。

こういうの、実際にやって自分で気付けるのもなかなか理解深いな、と自画自賛。

### 例12.2.3のサイコロn投げる例がめっちゃ凄い！

サイコロをn個投げて目の合計がアルファでした、1の目のサイコロ、2の目のサイコロ、、、はそれぞれいくつずつでしょう？という問題。
こんな具体的な簡単な問題でこんな奥深い物が見れるのか、と感動した。

これを単純に、合計を元に、それが達成されうる各目を考えてその組み合わせを考える、という二重のfor文で解くのは簡単に出来るが、人間にはちょっと解けそうも無いし、それではなんの洞察も得られない。

各目がいくつ出るかを固定した時の組み合わせの数を、制約条件を元に最大化する、と考えるとこの問題は美しく解ける。この発想の転換は面白い。

これが12.19としてもとまるだけじゃなくて、このpスターのそばにほとんど全部の場合の数が集まる、というのも味わい深い。

こんな風にこの問題を考えられたらかっこいいなぁ。

### 12.2はなかなか面白かった

例がつらつら並んでるだけの節だが、思ったより面白い。
ちゃんと全部を消化出来てるほどは自分のものに出来てないが、 maximum entropyは一見すると雑すぎる近似に思えるが、含意はなかなか深い。
この節は読む価値があった。

## 12.6 burgの定理
 
少し間があいた（といっても数日だが）のでいろいろ忘れてる。復習しつつ読み進めたい。

つらつらと読んで行った所、Yule-Walkerの定理の所でわからなくなったのでメモを書いておく。

別段この話題に興味がある訳でも無いので、難しそうなら飛ばす、くらいの気分で。

### Yule-Walkerの定理

Rがなんなのかが良く分かってない。
covarianceは普通行列で、要素は足が2つだと思うのだが、0からpまでのcovarianceとは一体？

読み直して見ると、12.4のスペクトル推計の所で同じ記号が出てきている。
これは何故iに依存しないのだろうか？
それはstationaryという仮定からか。 

つまりR(k)は、k個先のデータとの相関、という意味だな。

12.6に戻ると、stationaryだと時間のシフトで結果が変わらないので、p次のマルコフ過程ならある時点から前p個との相関で全covarianceが尽くせるな。

少し背景知識が足りない気がしたのでyoutubeの動画を見てみた。

[youtube: Autoregressive Models: The Yule-Walker Equations](https://www.youtube.com/watch?v=PFyp4t16_xk)

12.53までは割とちゃんと理解出来た気がする。

12.55が良く分からないな。
計算してみよう。

![images/2019-03-22-135325/0007.jpg]({{"/assets/images/2019-03-22-135325/0007.jpg" | absolute_url}})

あれ？惜しいけど絶対値が出てこない。
多分なにか間違ってるんだろうけど、だいたいどういう感じかは理解出来たからいいか。

## 12章はなかなか面白かった

もともとME自体を知りたいというモチベーションはサイコロ本などですでに高かったので、割と楽しく読めた。
また、前の章のtypicalとかの話を前提にMEを考えるといろいろと違う視野がひらけて、ある種の感動がある。
学問とはこうありたいものだ。

後半のautoregrresive modelの話は、ちゃんと確率過程の教科書で勉強してから読まないとしっかりとは理解出来ないなぁ、と思った。
機会があったら勉強したあとにまたこれを読み直したい。

