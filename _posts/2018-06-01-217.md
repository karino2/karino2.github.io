---
title: "Attention is All You Needのメモ"
date: 2018-06-01 01:30:54
---

流行り物という事で論文を読んでみた。あまり理解出来てない。
でも勉強会で別の人が当番でこの論文の話をしてくれるので、質問出来る程度の理解まででいいか、という事で、そのメモをここに書いておく。

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

本家のソースは[Tensor2Tensor](https://github.com/tensorflow/tensor2tensor)らしい。
models/transformar.pyにモデルがある。
読みにくいが難しくはない。

attentionが自分の知ってる物と違ってweightを学習してなかったので少し復習。

これは以下のdot productのattentionの亜種っぽい。

[Effective Approach to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)

へー、これならたしかにNNで学習する意義は感じないね。

multihead attentionは論文とソースがだいぶ違う気がするんだが、よく理解出来てないのかなぁ。

もとのはsplit_headsでchannelをnum_headsに分割してるだけに見える。
この効果はちょっと考えたくらいでは良くわからんので、教えてもらおう。

追記: 菊田さんのレポジトリ

https://github.com/yoheikikuta/annotated-transformer/tree/cpu
