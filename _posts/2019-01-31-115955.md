---
title: "Elements of Information Theoryを読む"
layout: page	
---

そろそろ情報理論を真面目にやるか、と決意して、以下の本を読み始める。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=B00HLG9ISQ&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

これはGoodfellow本から参照されてた二冊の本のうちの一つで、サイコロ本でも確か言及されてて、Kindle版があった、みたいな基準で選んだ。

## 1.1 Mutual Informationの計算

Example 1.1.3とかをやっておく。
まず必要な事を簡単にメモ。

![images/2019-01-31-115955/0000.jpg]({{"/assets/images/2019-01-31-115955/0000.jpg" | absolute_url}})

最後の式は簡単に確認しておくか。

![images/2019-01-31-115955/0001.jpg]({{"/assets/images/2019-01-31-115955/0001.jpg" | absolute_url}})

conditional entropyは式2.10に定義がある。
条件付き確率の期待値なので同時分布の和になる。

### ノイズなしバイナリチャンネル（Ex 1.1.3）

information capacityは1ビットとなる、と言ってるが、定義に従い計算してみよう。

まず、xが0か1を等確率で送る場合の計算をしてみる。

![images/2019-01-31-115955/0002.jpg]({{"/assets/images/2019-01-31-115955/0002.jpg" | absolute_url}})

結局p(x|y)はいつも1か0で、0の時は同時確率がゼロだから寄与しない。
という事でいつもxのエントロピーと一致するのか。

ではエントロピーが最大になるxは？という事になり、これは変分法で凸関数がどうとかになる話と思うが、離散的な上くらいの例なら1となる確率をp、0となる確率を1-pとしてpで微分、とかで出そう。
実際p(1-p)はpが1/2の時最大だろう。 

（追記: Example 2.1.1に解説があった）

なるほど、xの分布を変えて最大値を狙う、というのがinformation capacityという概念か。

### バイナリ対称チャンネル（ex 1.1.5）

これも計算してみよう。xは等確率で0か1で良かろう。
とりあえず必要な表を書く。

![images/2019-01-31-115955/0003.png]({{"/assets/images/2019-01-31-115955/0003.png" | absolute_url}})

なるほど。これを使って定義に従って計算してみる。

![images/2019-01-31-115955/0004.jpg]({{"/assets/images/2019-01-31-115955/0004.jpg" | absolute_url}})

なりそうだね。

