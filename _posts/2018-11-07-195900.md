---
title: "サイコロ本（Foundations of Statistical Natural Language Processing）を読むぞ！"
layout: page	
---

BERTがなかなか良さそうなので、次はNLPやろうかな、と思い、サイコロ本を読んでおこう、となる。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262133601&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

自分の認識だとこの本は定番の教科書だがニューラルネット以前の奴という位置づけ。
まぁembeddingsとかencoder-decoderとかは知ってるので、むしろこういう奴の方が自分には必要かな、と思い読む事に。

### 自分の前提知識

自分がこれまで読んだ本。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4339027545&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

昔機械翻訳の研究をやろうか、と思った事があって、研究室を選ぶ為に教科書を学ぼう、と思った事があった。
seq2seqとかより前の頃。
で、この教科書読んでたら、自分がドクター取る頃にはこの問題片付いてそうだな、と思ってやめた。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4061529048&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

トピックモデルが知りたくてこの本読んだ事がある。一応ちゃんと理解した（もう忘れたが）

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4061529242&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

ちゃんとRNN使った自然言語処理を理解したいと思って読んだ。結講真面目に読んだ気がする。

その他seq2seqとその周辺の論文とか、word2vecとかは一応読んで、だいたい理解してたと思う（もう忘れたが）

またグラフィカルモデルは結講真面目に勉強してて、HMMとかは実装も数学的な話は証明とかも割と真面目にやってて結講得意。

という事で何も知らないという訳でも無いがちゃんと勉強した、という訳でも無い状態なので、基礎をちゃんと勉強してみよう、と思った次第。

ただこの辺の式追うのはそんなに困ってないので、式変形とかを頑張って追うかは未定。読んでから考えます。

### Road map

全4部構成で、Part1は数学とかlinguisticとかの入門らしい。さすがに飛ばし読みでいいか？

Part2はWord。
ここが一番真面目に読みたい所かな？
collocation、disambiguation、attachment disambiguitiesとかが大切らしい。たぶん知らなさそう。

Part3が文法。
最近あんま見ない話題ね。

Part4が応用。
IRとかテキストのカテゴライズとかの話らしいので、意外と興味深いかも。

# 1章 イントロダクション

序盤の真の文法みたいなルールでは無くただ存在するテキストを確率的に扱うjustificationの所が難しい。
前置詞がどうとかの文法用語にあまり慣れてない上に英語のきわどい使い方についての話が多いので、自分の英語力では判断出来なかったり何を言いたいのか分からなかったりする。

英語力不足は仕方ないのでこの辺は読み飛ばすかなぁ。
そして現代機械学習屋としては、empiricismへの批判というのが良く理解出来ない。
確率モデルは簡単な事しか扱えないと誤解されがちとか言われても、そんな誤解してる奴はもはや居ないのでは…

なお、Corpusはテキストのbodyの事だ、と書いてある。bodyというのは本文という意味かね。
なんとなく語彙の辞書みたいな意味と思ってたが全然違った。

パースの方法とか単語のdisambiguationがどうとか言ってるが、最近のBERTは割とWittgenstein的な単語の意味とか文脈をそのまま学習してて、こういう要素は無い気がする。

こういう一つ一つを正しくやるのが極めて難しい物を積み重ねるって、いかにもあんまうまく行かなさそうだよな。

一回しか出ない単語をhapax legomenaって言うらしい。ギリシャ語。何故。

一回しか出ない単語が多い、という話。
この前見た論文はトリグラムのベクトルで単語を表してたな。bertはこの問題にどうアプローチしてるのかしら？

### Zipf's law

おお、ようやくなんか知ってた方が良さそうな話になってきた。
そうそう、こういう背景知識みたいなの知りたくてこの本読み始めたのだよね。

こういう既知の、言語に関する洞察は知っておくと良い事ありそうたよな。
この周辺の話は面白い。

## 1章を読み終わって

まず最初の印象として、英語の文法の知識が足りないな、という物。
文法用語も知らないし、英語のややこしい用法とかも分からない。
言葉的な事で勝負するならちゃんと英語の文法書とかは勉強した方が良いな、とは思った。
一章が特別そういう記述が多い、というのを期待したいが。

また、現代に役に立たなくなった事も多そうだな、という所。
これから学ぶという時に要らない物を判断する、というのは難しさもあるが、
要らなさそうな所で面倒な所は適度にサボって進めたい。将来何が役に立つかは分からない、という部分はあるが、費用対効果はやはり考えたい。

ただ現代でも役に立つ事も多く載っているな、とも思った。


# 2章 数学

確率論とかの話。シグマ集合体とかがinformalに導入されて、へーとは思ったが、さすがに知らない事は無いかなぁ。


### Example2を解いてみる

簡単なベイズ統計の入門の話題だが、
たまにはやらないとカンが鈍るのでやってみる。
テストが陽性なのをテ、parasitic gapがある事をパと書く。

なお、parasitic gapがなんなのかは知らない。

![images/2018-11-07-195900/0000.jpg]({{"/assets/images/2018-11-07-195900/0000.jpg" | absolute_url}})

うむ、さすがにこの位は解けるな。よしよし。

### エントロピー Example9

いい練習という事でエントロピーの計算をやっておこう。

まずH(C)を出す。

![images/2018-11-07-195900/0001.jpg]({{"/assets/images/2018-11-07-195900/0001.jpg" | absolute_url}})

ふむ、問題無いな。次にV given Cのエントロピーを出そう。

![images/2018-11-07-195900/0002.jpg]({{"/assets/images/2018-11-07-195900/0002.jpg" | absolute_url}})

各場合で1に規格化しないと駄目なのね。

次に本では、H(C, V)を積の法則に相当する式で出している。
まずはこの式を自分でも証明しておこう。

![images/2018-11-07-195900/0003.jpg]({{"/assets/images/2018-11-07-195900/0003.jpg" | absolute_url}})

最後の等号は定義のような物だが、いまいちピンとこないので少し考えておく。

![images/2018-11-07-195900/0004.jpg]({{"/assets/images/2018-11-07-195900/0004.jpg" | absolute_url}})

真ん中の式は中括弧内の期待値になってる。
各Cの値での条件付きエントロピーの期待値になってる訳だ。なるほど。

積の法則に対応するchain ruleは、ようするにCを送ってからCの値を使ったエンコードでVを送る時の平均ビット数、と考えると納得は出来るな。

さて、同時エントロピーはせっかくなので本とは別の方法で導出してみるか。
定義に従い、同時確率で出してみる。

![images/2018-11-07-195900/0005.jpg]({{"/assets/images/2018-11-07-195900/0005.jpg" | absolute_url}})

うむ、特に問題無いな。

### Entropy rate

唐突にサブスクリプトでサブシーケンス表すノーテーションと同時に導入されるので分かりにくいが、単語がn個来る時の、n単語の同時エントロピーをnで割った物か。

同時エントロピーのランダム変数をどんどん増やすとどんな感じかイメージするのは難しいが、何かその極限が表す概念に意味がある事があるのだな。
これは先に出てくるのだろう。

### 2.2.3 Mutual information

初めて見る気がする話だな。ただなんか見覚えはあるので昔どっかでやったのかもしれない。
何にせよ覚えてはいないな。

情報理論っていまだにちゃんとやった事無いんだよな。
そろそろやっても良い頃な気はする。

この本でもCover and Thomasが薦められてるな。Goodfellow本でもこの本が薦められてたのでこれ読むかなぁ。

### 2.2.4 noisy channelと翻訳

おぉ、Encoder-decoderモデルっぽい！
本来はこういう背景を元にRNNで実装したのがEncoder-decoderモデルなんだろうけど。

逆にこういう定式化の汎用性の高さは、汎用noisy channelモデルが出来たらこれだけの応用がある、という事だよな。
それは数年以来に来そうな未来だやね。
楽しみだ。

### 2.2.6 cross entropyの話（が分からない）

式2.48がいまいち分からない。H(L, m)という奴。Lがなんなのか分かってないな。

まず定義式を真面目に見よう。

![images/2018-11-07-195900/0006.jpg]({{"/assets/images/2018-11-07-195900/0006.jpg" | absolute_url}})

良く分からないので、limitを取る前の適当なnについて考えてみよう。

![images/2018-11-07-195900/0007.png]({{"/assets/images/2018-11-07-195900/0007.png" | absolute_url}})

和は何について取ってるか？というと、X1, X2, ... , Xn までのランダム変数のシーケンスだよな。

ややこしいが、各ランダム変数は単語を表す実数値なのだろう。
ようするにこれで単語がn個連なった文を表す訳だな。

pの中もmの中も同時分布を表しているのだろう。
だから前後関係とかは関係ある。

シグマの中を考えてみよう。
pはそんな単語列の文が現れる確率だな。
これはたぶん長さnの文、というのを母集団としててその中で規格化されてるのだろう。

logの中もpだったらこれは単なるエントロピーだな。
それを言語モデルmに置き換えてるのだから、ようするにpとmのクロスエントロピーか。

あー、文とは限らないか。むしろ文と文をつなげて、そういう文が連続すればOKなんだな。とにかくどこまでも続く単語の列を見ていく訳か。

凄く大きなNという数字でこれを考えると、なんかずーっと続く文をN単語目まで見る、をいろんな文書などで繰り返す訳だ。
で、それらの単語の列がそれぞれ現れる可能性をpとして、言語モデルでその単語の列が現れる可能性をmとしてそのクロスエントロピーを出すのだな。

さて、次の2.49は、niceな性質なら成立する、と言ってるがniceってなんやねん。

とりあえず式の意味を考えると、十分に長い一文の同時確率と、十分に長い単語列をいろいろ集めた時の同時確率の期待値が一致する、と言っている。
つまり十分に大きな数Nを固定して、このN個の単語列をたくさん集めると、どの単語列の出てくる確率も一緒と言ってるんだな。

それが何を意味するのかは分からないが、そんな不自然な事でも無い気はする。

追記: あとでこのniceな性質に言及があって、Shanon-McMillan-Breimanの定理と言うらしい。

### 2.2.8 Perplexity

分からない記述があるのでメモを残しておく。
2.53の右辺は以下のように、Hの中は小文字になってる。

![images/2018-11-07-195900/0008.png]({{"/assets/images/2018-11-07-195900/0008.png" | absolute_url}})

こんなのの定義はあったっけ？と前を見直したが見つけられなかった（探し方が不十分かもしれない）。

確率変数に対しての定義は、式2.46にある。
書いておくと、確率分布pに従う確率変数Xと、pとは別のpmfであるqに対して、Xとqのクロスエントロピーとは以下の式で定義される。

![images/2018-11-07-195900/0009.png]({{"/assets/images/2018-11-07-195900/0009.png" | absolute_url}})

これは普通のクロスエントロピーの式だが、Xとqで表記されてるので注意が必要。

さて、式2.53の右辺はこのポイントワイズ版に見える。
2.54式から逆算して考えると定義は以下か。

![images/2018-11-07-195900/0010.png]({{"/assets/images/2018-11-07-195900/0010.png" | absolute_url}})

あ、式2.50の右辺だな。この式にこういうノーテーションを与えたのかな。
これは式2.50によればLとmのクロスエントロピーの近似値だ。

つまり2.53はLとmのクロスエントロピーの近似値とperplexityの関係か。

perplexityがkの時は、基本的に各単語を、毎回「次はこのk通りのうちのどれかだな」と思って見てる事に相当するらしい。

2.54の右辺をk乗して、積の法則で一文字ずつ見ていく時の条件付き確率の積とすればそうなってるな。

## 2章読み終わり。勉強になった！

最初は読む意味無いかな、と思ってたが、Information theory周辺はとても勉強になった。Information theoryやらんとのぅ。

perplexityとか良く評価の所で出てくるが分かってなかったので、今回ちゃんと理解出来て良かった。

2018年現在でも、なかなかこの本は読む価値があるな。続きも楽しみ。

# 3章 Linguistic Essentials

英語の文法の専門的な話で、予想以上に辛い。
話題にしている内容が英語の特殊な用法とかだったりするので、そもそも知らなかったり、知っててもあまり詳しくはない場合もある。

文法用語も知らないのが多く、非ネイティブは不利な分野だなぁ、とか思う。

ただ意外と義務教育でやった英語はこういう話が多く、全く歯が立たない訳でも無い。
英語の文法の参考書とかって向こうの文法書とかを参考に作られてたんだね。当たり前か。

英語の勉強と前向きにとらえて頑張って読む。

### 3.1 品詞とか

Parts of speechと言うらしい。
名詞とか形容詞とか動詞とかをいろいろ説明し、Brown tagでは何が割り当てられているか、を説明している。
細かい分類は良く理解出来ているとは言い難いが、どんなタグが割り振られているかの雰囲気はつかめた。
大変辛かった。

### 3.2 文の構造的な話

文法の木のあたりで略語が良く分からなくなってきたのでメモ。

NPはnoun phrase、VPはverb phrase、PPはprepositional phrases、APはAdjective phrase（形容詞句か）。

さらに前節のBrown tagが混ざってるのか。Table 4.5に一覧があるのでこれを見ながら頑張るのね。

文法用語に慣れてきたせいか受験英語程度の文法知識で対応出来るからか、とにかく3.2の方が読めるな。
文法用語の勉強として読むか、と自分の中にモチベーションが作れたせいかもしれない。

### ドメインの知識について

SVMくらいまでの機械学習というのは、ドメインの知識をモデルに入れ込むのが重要だった気がする。
サイコロ本の三章などもそういう事の基礎に思う。

一方でXGBoost以降だと、ドメインの知識を学習出来るように問題設定を考える方がよくなったよなぁ。

BERTやencoder-decoderなど、ドメインの知識はあまりモデルの構造には入ってないし、
そちらの方が筋が良い、と我らも感覚的に感じる。

なんかそこには時代の違いがある気がするね。名前つけても良さそうだが。

# 4章 Corpusとかプログラムとか

3章の辛さが嘘のようにスラスラ読めるな。
エディタはemacsがいいぜ、とか、正規表現便利だぜ、とか、Cは速くていいがPerlも前処理とかには楽だぜ、とか、話題が突然知ってる話になるからだな。

### 4.2 トークナイズの話とかは辛い

コンピュータに扱うのが難しいケースは非母国語話者にも難しいな…と思う例が多い。
難しい英文をたくさん読むので大変疲れる。
しかも幾つかは分からなかったりもする。

NLPの話では無いと思うがこういうのはサイコロ本辛いよなぁ。

# 5章 コロケーション

英語の例文の難易度は相変わらず高くて辛いが、コロケーションという現象とそれをどう扱うかという話自体はそんなに英語の文法知識は要らないので、3章や4章よりは大分楽に読める。

### 5.3.1 t検定

おぉ、コロケーションの判定の為に、t分布で検定とかしてる。
なんかこういう昔ながらの統計の話って最近まったく見ないよねぇ。
練習として解いてみても良いかと思ったが、必要な数がどんどんあとから出てくるからやめ。

基本的なアイデアとしては、

1. 単語の頻度からそれぞれの単語の出る確率を出し、積を同時確率とする
2. new companiesの出現をバイグラム群からのこの確率によるベルヌーイ試行とみなして平均、分散を計算
3. 実際の登場確率をサンプル誤差としてt検定を行う

という感じ。

なお、そのあとにstop wordを消すとだいたい有意になる、みたいな話が書いてある。
単語はランダムじゃないからだ、みたいな事言ってる。
で、検定で有意かどうかじゃなくてランキングとして使え、と書いてあって、それの統計的意味付けとは…とかいう気分になる。

これなら普通に事後確率をスコアとする方がクリアだよなぁ。

### 5.3.3 ピアソンのカイ二乗検定

なんか懐かしい物が続くな。
いい機会なので昔勉強した統計の入門書の該当箇所を読み直すなどする。
統計は自炊するようになったあとに本格的に勉強しはじめたので、この辺もギリギリ電子化されてるのだった。

と思ったが式5.7は無いな。導出するか。

![images/2018-11-07-195900/0011.jpg]({{"/assets/images/2018-11-07-195900/0011.jpg" | absolute_url}})

![images/2018-11-07-195900/0012.jpg]({{"/assets/images/2018-11-07-195900/0012.jpg" | absolute_url}})

う、これはなりそうな気がするが、めんどくさいな。
まぁ頑張るか。

![images/2018-11-07-195900/0013.png]({{"/assets/images/2018-11-07-195900/0013.png" | absolute_url}})

頑張った。なりそうね。
なんか昔証明したことあるな、これ。

### tfのカイ二乗スコアで類似度を測る

こんなやり方あるのね。知らなんだ。
ただこれだけだとその言語の比率を表してる場合もあるので、一般的な比率とも比較しないといまいちな気がするが。

なんにせよ、corpusの類似度は現代でも興味深い問題ではある。

### 5.3.4 Likelihood  ratios

Likelihoodは慣れたもんだが、Lがなんなのか分かりにくいね。
これはそれぞれの前提で、w2がc2回観測される尤度なんだな。

L(H1)は、

- 「c1個のw1から、c12が得られる尤度」
- 「N-c1個のそれ以外の単語からc2-c12が得られる尤度」

で、その積でw2がc2個得られる尤度を求めてる訳だ。
L(H2)も同様。
pやp1, p2から二項分布を仮定して尤度出す所と、このLがなんなのか、というところで二箇所難しい場所があるので、この解説読んでさくっと理解するのはなかなか難度高いな。

c1は所与というか、その尤度は考えてない。当然2つの仮説で同じだから正しいのだけど、ちょっと説明無いのは不親切ね。

### 5.4 Mutual information

5.11でpoint wiseなmutual informationの定義が出てくるのだが、そんな値が何に使えるんだ？というのが良く分からない。

で、先を読んでくと、やはりいまいちでその理由が書いてあるが、そもそもこの値を何を期待して使ってたのかの方が良く分からない。

結論としても、uncertaintyのreductionの度合いは別に興味深いコロケーションを示唆する訳じゃない、と言ってて、定義からそりゃ明らかだろう、という気もしてしまう。
わざわざそんな事に言及するのは、なんかこういうのが誤って良く使われてた時期があるのかね？

## 5章読み終わり

この辺から英語の文法のお勉強感は減ってくるね。
コロケーションはskip-gramとかと近い話なので割と馴染みはある。

やけに検定にこだわる所が最近の本とは違うなぁ、と思うが、結果を単にスコアとしてソートするなら検定の枠組みにこだわる必要は無いのでは…
別にカイ二乗のスコアが高い方が関連が深いことを意味する訳じゃ無いのだし。

そういう訳で解法はどれもあまり凄いアイデアとは感じなかったが、問題を理解しておくのは現代でも有意義と思うので、読む価値はあった気がする。

ついでに検定の復習にはなった。最近検定とかやらんからね。

# 6章 言語モデル

n-gramで言語モデル作ってみる、という話。
この辺は自分でも同じ事やった事はあるし別に難しい事は無い。

## 6.2 n-gramの言語モデルいろいろ

出てこなかっかバイグラムが多すぎてナイーブに扱うと問題が出る、という話から始まり、ふんふん、と読んでいく。

なんかいろいろなモデルが出てくるが、何故か導出が無い。適当なprior置いて簡単に定式化出来そうなのに。
なんかそういうの真面目にやる本と思ってたが違った？

まぁどれも割と素朴なモデルなのでいいか、と読んでいくと、最終的にはGood Turing Estimatorがいいぜ、と言ってくる。

なるほど、と読んでいくと、スムージングしたカーブをなんかアドホックにつなげる、とあるがいまいち詳細が良く分からない。
そのあと具体例で計算してみるぜ、と続くので気合入れて読むか、と思ったら、
実際の計算はwebにあげておいたプログラムでやったぜ、とかいって解説が無い…

えー！？詳細はwebで！って奴！？そりゃ無いよ…

### Good Turing Estimatorのコードを読む

6章のwebは[ここ](https://nlp.stanford.edu/fsnlp/statest/)か。
そこから[Cのコードへのリンク](https://nlp.stanford.edu/fsnlp/statest/SGT.c)が貼られている。

最初は入力をテキストと思ってたので意味が分からなかったが、どうも入力は、文中で言う所のrとNrを一行につきスペース区切りでひとつずつ書いた物ってぽい。
rの昇順っぽい。

あとで読み直す時の為に軽く説明を書いておくと、rはCorpus内でn-gramが何回出てくるかを表す。

例えばabが10回出てくるならrは10となる。
Nrはrがr回のn-gramの数。
つまり、N_10なら、10回出てくるn-gramが幾つあるか、という事。

readValidInputではこれらの空白区切りの数を読んでいって、配列rに何回出てくるかを、配列nに文中のNrを入れている。

rはだいたい0, 1, 2,と順番になりそうだが、理論上は5回出てくるn-gramと7回出てくるn-gramはあっても6回は無い、という事がありそう。

で、これでセットアップした配列を使ってanaylseInputを呼ぶ。
これが実際の仕事をする関数っぽい。

bigNはCorpus内の、重複もカウントしたn-gramの個数だな。
PZeroはなんだろう？
一回だけ出てくるn-gramの比率に見えるが…

row(1)という関数はrが1のインデックスを返しそう。普通に考えれば1なのでは？
（データが猛烈に多い場合のバイグラムとかならrが0な物が無い場合もありえるかもしれないが）。

そう考えるとPZeroはN1/Nに見える。
N1はNrのrが1の物という意味ね。

次のfor文ではlog Zとlog rをキャッシュしていく。
log rはいいとして、Zってなんぞや？

コードが読みにくいが、端以外では、iは一つ前のr、kは一つあとのrが入っている。
つまり、

$$Z_j = \frac{2 \times N_j}{r_{j+1}-r_{j-1}} $$

っぽい。（注: 今サイトのバグで数式が出てません。来週日本帰ったら直します）

分母は普通2だよな。するとzは普通はNiと一致するのか。
どういう時一致しないか？というとrが飛び飛びの時だよな。

例えばr=5で、その下が4、その上が7としよう。
するとこの分母は3となる。

N5を2/3した物という事になる。

![images/2018-11-07-195900/0014.jpg]({{"/assets/images/2018-11-07-195900/0014.jpg" | absolute_url}})

こんな感じになるが、このZはなにをしたいのだろうか。
最終的にはrからNrをもとめる回帰式を求めたいのだよな。

分からん。

findBestFitを見るとlog rとlog Zの単純な線形回帰をしているようにみえる。

抜けてる所を何かしてるように見えるが、回帰のx側はrを使ってるので、4, 5, 7, 8のままなのだよな。

5と7が少し小さくなるみたいだが何故だろう？分からん。

仕方ないのでそのまま進む。
smoothedが先程いったように線形回帰した結果。

281行目のyは教科書にもある推定値だな。

その次の283行目のif文とindiffValsSeenとはなんだろう？

rowという関数は、引数の値と同じrを持つrowのインデックスを返す。
rj+1が無いとは、この次が抜け番の場合か、最後の場合の処理だよな。

xはSrじゃなくてNrを使った場合の推計値か。
if文の条件は読みにくくて泣いちゃうな。

![images/2018-11-07-195900/0015.jpg]({{"/assets/images/2018-11-07-195900/0015.jpg" | absolute_url}})

だいたいこんな感じか？
この最後の式が、回帰版の推計値のyと実測値版の推計値xとの差より大きいとxを使うらしい。

この条件がどこから出てきたかは良く分からないが、本文でrが小さい時は推計値じゃなくて実測値を使え、と書いてある、その条件なのだろう。

ソースの概要を理解して、細かい所に分からない所はあるがだいたいどう実装するかは理解した。先に進もう。

追記: 6章の最後を見てたら、注12にこの式とおぼしきものが載ってた。
これは標準偏差との事。
教科書の数値は1.65となっていてコードの1.96とは違うが、まぁそこはいいだろう。

なんでこれが標準偏差になるかはちょっと考えただけでは分からなかったが、それっぽい形なので導出は頑張らないで鵜呑みにする。

### 6.21式のメモ

式の意味が良く分からなかったがあとの例を見て分かったので解説を書いておく。

分子は対象とするn-gramの出現回数で、分母はそのn-gramの次の単語がどれくらい多様かを表している。

of thatは178回出ているが、of thatの次の単語はバラバラで115通りある。
つまりこのバイグラムのあとに文法的にありえない、という単語はあまり無い。
この2つの数を割った178/115は1.55。

一方great dealも178回だが、その次の単語はより限られていて36種類しか無いらしい。
だからこのバイグラムのあとは文法的にありえない組み合わせが多そう。
178/36は4.94。

この1.55とか4.94に応じてグルーピングし、それぞれのグループごとに一つのラムダを割り当てて、それらの値を6.20式としてEM法で最適化する、という事だと思う。
historyをこのグルーピングの値を計算するのにだけ使う、という事か。

### イマドキのスコアと比較してみよう

n-gramの洗練されたモデルにより、perplexityが240という洗練したスコアを達成した！

という事でイマドキのモデルでのスコアを冷やかしてみよう。
同じデータセットで比較しろよ、と言われそうだが旅先だしね。

で、bertはそれっぽい物を見つけられなかった（Table 6のpplは4前後っぽい気もするがテストセットでのスコアじゃないよな、これ）ので、一つ前のOpenAI GPTで。これもTarnsformer時代の相当強いモデル。

で、この人達のcorpusでは18.4だって！
一桁小さいね。
次の単語を予測する時、だいたい平均18択くらいには絞れる、という事らしい。
ほんとにそんな低いのかな？試してみたいね。

## 6章のn-gramモデルを読んで

バイグラムの分布をかなり正しく予測出来る、というのは分かった。
モデルはかなり原始的で、変な調整が入ってるにせよ、単純にr回出現するn-gramの数から割とストレートに予測している。

これはこれで一つの結果とは思うが、言語モデルとしては貧弱だなぁ、という印象も拭えない。
結局n-gramでは、大半は見たこと無い物になってしまうので、良くある物とユニグラムの組み合わせみたくなっちゃう。

単語の前後関係から何かを学習してくれないとねぇ。

ただ単純なモデルなので、問題を理解する為のスタート地点としては良いね。
やはり最初は簡単な問題から始めないとね。

# 7章 Word sense disambiguation

複数の意味のある単語のうち、どの意味かを当てる、みたいな話。
こうやってここの要素を積み上げるって2018年の感覚だといかにも上手く行かないやり方だが、教養として知っておく事には意味はありそう。

### 7.2.1 ナイーブベイズ

ふんふん、と読んでたらナイーブベイズが。懐かしいな。
ただどこでやったのかが少し調べたが出てこない。たぶんCourseraのPGMかな。
他でもやった気がするが…

やはりdaphneのPGM本には載ってるな。
この本はadobe DRMでビュワーが腐ってるから開くの嫌なんだよな…

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262013193&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

今だとKindle版があるので世界は平和だね。
3.1.3.2がナイーブベイズ（p49）。

とりあえずグラフィカルモデルを書いておこう。

![images/2018-11-07-195900/0016.png]({{"/assets/images/2018-11-07-195900/0016.png" | absolute_url}})

ナイーブベイズは、目的の事後確率である条件つき確率の、条件がcondititionalyにindepenentという事かね。
この時のcondititionalyはベイズルールで反転した、つまり目的とするskの事だ。

お、式3.6でそう書いてあるな。

![images/2018-11-07-195900/0017.png]({{"/assets/images/2018-11-07-195900/0017.png" | absolute_url}})

この仮定を置くと、単純な各単語の尤度の積になるのでバイグラムをカウントするだけで簡単に推計できる。

この頃のモデルは簡単でいいね…

### 7.2.2 Information theoretic approach

お、例のフランス語の意味が分かる！
フランス語の多義語の話をしてるが、ここはフランス語知らないと分かりにくいな。ふふん、俺様教養人なのでこのくらいの単語は分かるぜ。
という事でちょっと分かりやすくて得した気分。

このmutual informationというのはあまり良く理解してないが、式の意味は確率論的にとらえてもここでのアルゴリズムは問題無さそうなので深く考えずに読む。

具体例を挙げてくれているのでこれを追ってみよう。
まず前提から。

prendreの意味を推測するタスクを解きたい。
候補はt1からtmで、この場合はtake, make, rise, speakのどれか。

で、ヒントになりうる単語はx1からxn で、この場合はmeasure, note, example,. decision, paroleのどれからしい。

paroleは知らない単語だな。
辞書を引くと「言葉、発言」などの意味で、「prendre la parole」で「発言する」という慣用句らしい。へー。知らなんだ。

なお、英語もrise to speakで立ち上がって話しかける、みたいな慣用句らしい。こちらも知らんがな。

事実をまとめると以下だな。

![images/2018-11-07-195900/0018.png]({{"/assets/images/2018-11-07-195900/0018.png" | absolute_url}})

で、FlipFlopアルゴリズムはまず赤文字を2つのグループにランダムにわけて、これをP1, P2と呼ぶ。 

最初は

- P1 ... take, rise
- P2 ... make, speak

と分ける。riseとspeakが別なのは残念だが、ランダムなのでそういう事もある。

で、次にこれとのmutual informationが最大になる青文字の分割を考える。
つまりヒントとして何があれば一番P1とP2のどちらか見分けやすいか、と考えれば感覚的にはいいかな。

riseとspeakはどうせ区別出来ないので、take とmakeだけ考えると、

- Q1 ... measure, note, exemple
- Q2 ... decisision

で、parole どっちでも良い。テキストではQ2に含めている。

次にこのQ1とQ2を固定としてPの方の分割を見直す訳だ。
これはQ1から確実に推測できるグループとそれ以外、という分割になり、P1はtake、 P2はそれ以外になるな。

なんかEM法っぽいね。

このやり方だと2グループのケースしかやってないが、確実に独立な物が作れればそれを取り除いて再帰的にやれば良さそうだし、何か工夫もあるかもしれん。

そのあと、この分割方法はexponential timeがかかっちゃうが、もっと効率的に分割を探すアルゴリズムがある、と言って詳細は説明してない。
意外とぬるい本だな。ゆとりには安心。

### 7.3.1 辞書を使ったambiguation

Vjという物の定義がわかりにくかったのでメモ。（数式はブログのトラブルで出てない。後で直す）

$$V_j$$とwの近いが分かりにくい。
$$V_j$$はw以外の一般の単語、という事か。
例えばcの中の各単語、とか。

すると $$E_{V_j}$$ はこの$$V_j$$の辞書での意味のunionか。

例えばTable 7.4では、$$V_j$$は、Thisとかcigarとかburnsとかを表すのかな。
このテーブルには$$E_{V_j}$$ は無い気がする。
$$V_j$$と$$D_k$$のunionをカウントしている気がする。

