---
title: "サイコロ本（Foundations of Statistical Natural Language Processing）を読むぞ！"
layout: page	
---

BERTがなかなか良さそうなので、次はNLPやろうかな、と思い、サイコロ本を読んでおこう、となる。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262133601&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

自分の認識だとこの本は定番の教科書だがニューラルネット以前の奴という位置づけ。
まぁembeddingsとかencoder-decoderとかは知ってるので、むしろこういう奴の方が自分には必要かな、と思い読む事に。

### 自分の前提知識

自分がこれまで読んだ本。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4339027545&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

昔機械翻訳の研究をやろうか、と思った事があって、研究室を選ぶ為に教科書を学ぼう、と思った事があった。
seq2seqとかより前の頃。
で、この教科書読んでたら、自分がドクター取る頃にはこの問題片付いてそうだな、と思ってやめた。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4061529048&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

トピックモデルが知りたくてこの本読んだ事がある。一応ちゃんと理解した（もう忘れたが）

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4061529242&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

ちゃんとRNN使った自然言語処理を理解したいと思って読んだ。結講真面目に読んだ気がする。

その他seq2seqとその周辺の論文とか、word2vecとかは一応読んで、だいたい理解してたと思う（もう忘れたが）

またグラフィカルモデルは結講真面目に勉強してて、HMMとかは実装も数学的な話は証明とかも割と真面目にやってて結講得意。

という事で何も知らないという訳でも無いがちゃんと勉強した、という訳でも無い状態なので、基礎をちゃんと勉強してみよう、と思った次第。

ただこの辺の式追うのはそんなに困ってないので、式変形とかを頑張って追うかは未定。読んでから考えます。

### Road map

全4部構成で、Part1は数学とかlinguisticとかの入門らしい。さすがに飛ばし読みでいいか？

Part2はWord。
ここが一番真面目に読みたい所かな？
collocation、disambiguation、attachment disambiguitiesとかが大切らしい。たぶん知らなさそう。

Part3が文法。
最近あんま見ない話題ね。

Part4が応用。
IRとかテキストのカテゴライズとかの話らしいので、意外と興味深いかも。

# 1章 イントロダクション

序盤の真の文法みたいなルールでは無くただ存在するテキストを確率的に扱うjustificationの所が難しい。
前置詞がどうとかの文法用語にあまり慣れてない上に英語のきわどい使い方についての話が多いので、自分の英語力では判断出来なかったり何を言いたいのか分からなかったりする。

英語力不足は仕方ないのでこの辺は読み飛ばすかなぁ。
そして現代機械学習屋としては、empiricismへの批判というのが良く理解出来ない。
確率モデルは簡単な事しか扱えないと誤解されがちとか言われても、そんな誤解してる奴はもはや居ないのでは…

なお、Corpusはテキストのbodyの事だ、と書いてある。bodyというのは本文という意味かね。
なんとなく語彙の辞書みたいな意味と思ってたが全然違った。

パースの方法とか単語のdisambiguationがどうとか言ってるが、最近のBERTは割とWittgenstein的な単語の意味とか文脈をそのまま学習してて、こういう要素は無い気がする。

こういう一つ一つを正しくやるのが極めて難しい物を積み重ねるって、いかにもあんまうまく行かなさそうだよな。

一回しか出ない単語をhapax legomenaって言うらしい。ギリシャ語。何故。

一回しか出ない単語が多い、という話。
この前見た論文はトリグラムのベクトルで単語を表してたな。bertはこの問題にどうアプローチしてるのかしら？

### Zipf's law

おお、ようやくなんか知ってた方が良さそうな話になってきた。
そうそう、こういう背景知識みたいなの知りたくてこの本読み始めたのだよね。

こういう既知の、言語に関する洞察は知っておくと良い事ありそうたよな。
この周辺の話は面白い。

## 1章を読み終わって

まず最初の印象として、英語の文法の知識が足りないな、という物。
文法用語も知らないし、英語のややこしい用法とかも分からない。
言葉的な事で勝負するならちゃんと英語の文法書とかは勉強した方が良いな、とは思った。
一章が特別そういう記述が多い、というのを期待したいが。

また、現代に役に立たなくなった事も多そうだな、という所。
これから学ぶという時に要らない物を判断する、というのは難しさもあるが、
要らなさそうな所で面倒な所は適度にサボって進めたい。将来何が役に立つかは分からない、という部分はあるが、費用対効果はやはり考えたい。

ただ現代でも役に立つ事も多く載っているな、とも思った。


# 2章 数学

確率論とかの話。シグマ集合体とかがinformalに導入されて、へーとは思ったが、さすがに知らない事は無いかなぁ。


### Example2を解いてみる

簡単なベイズ統計の入門の話題だが、
たまにはやらないとカンが鈍るのでやってみる。
テストが陽性なのをテ、parasitic gapがある事をパと書く。

なお、parasitic gapがなんなのかは知らない。

![images/2018-11-07-195900/0000.jpg]({{"/assets/images/2018-11-07-195900/0000.jpg" | absolute_url}})

うむ、さすがにこの位は解けるな。よしよし。

### エントロピー Example9

いい練習という事でエントロピーの計算をやっておこう。

まずH(C)を出す。

![images/2018-11-07-195900/0001.jpg]({{"/assets/images/2018-11-07-195900/0001.jpg" | absolute_url}})

ふむ、問題無いな。次にV given Cのエントロピーを出そう。

![images/2018-11-07-195900/0002.jpg]({{"/assets/images/2018-11-07-195900/0002.jpg" | absolute_url}})

各場合で1に規格化しないと駄目なのね。

次に本では、H(C, V)を積の法則に相当する式で出している。
まずはこの式を自分でも証明しておこう。

![images/2018-11-07-195900/0003.jpg]({{"/assets/images/2018-11-07-195900/0003.jpg" | absolute_url}})

最後の等号は定義のような物だが、いまいちピンとこないので少し考えておく。

![images/2018-11-07-195900/0004.jpg]({{"/assets/images/2018-11-07-195900/0004.jpg" | absolute_url}})

真ん中の式は中括弧内の期待値になってる。
各Cの値での条件付きエントロピーの期待値になってる訳だ。なるほど。

積の法則に対応するchain ruleは、ようするにCを送ってからCの値を使ったエンコードでVを送る時の平均ビット数、と考えると納得は出来るな。

さて、同時エントロピーはせっかくなので本とは別の方法で導出してみるか。
定義に従い、同時確率で出してみる。

![images/2018-11-07-195900/0005.jpg]({{"/assets/images/2018-11-07-195900/0005.jpg" | absolute_url}})

うむ、特に問題無いな。

### Entropy rate

唐突にサブスクリプトでサブシーケンス表すノーテーションと同時に導入されるので分かりにくいが、単語がn個来る時の、n単語の同時エントロピーをnで割った物か。

同時エントロピーのランダム変数をどんどん増やすとどんな感じかイメージするのは難しいが、何かその極限が表す概念に意味がある事があるのだな。
これは先に出てくるのだろう。

### 2.2.3 Mutual information

初めて見る気がする話だな。ただなんか見覚えはあるので昔どっかでやったのかもしれない。
何にせよ覚えてはいないな。

情報理論っていまだにちゃんとやった事無いんだよな。
そろそろやっても良い頃な気はする。

