---
title: "サイコロ本（Foundations of Statistical Natural Language Processing）を読むぞ！"
layout: page	
---

BERTがなかなか良さそうなので、次はNLPやろうかな、と思い、サイコロ本を読んでおこう、となる。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=0262133601&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

自分の認識だとこの本は定番の教科書だがニューラルネット以前の奴という位置づけ。
まぁembeddingsとかencoder-decoderとかは知ってるので、むしろこういう奴の方が自分には必要かな、と思い読む事に。

### 自分の前提知識

自分がこれまで読んだ本。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4339027545&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

昔機械翻訳の研究をやろうか、と思った事があって、研究室を選ぶ為に教科書を学ぼう、と思った事があった。
seq2seqとかより前の頃。
で、この教科書読んでたら、自分がドクター取る頃にはこの問題片付いてそうだな、と思ってやめた。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4061529048&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

トピックモデルが知りたくてこの本読んだ事がある。一応ちゃんと理解した（もう忘れたが）

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4061529242&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

ちゃんとRNN使った自然言語処理を理解したいと思って読んだ。結講真面目に読んだ気がする。

その他seq2seqとその周辺の論文とか、word2vecとかは一応読んで、だいたい理解してたと思う（もう忘れたが）

またグラフィカルモデルは結講真面目に勉強してて、HMMとかは実装も数学的な話は証明とかも割と真面目にやってて結講得意。

という事で何も知らないという訳でも無いがちゃんと勉強した、という訳でも無い状態なので、基礎をちゃんと勉強してみよう、と思った次第。

ただこの辺の式追うのはそんなに困ってないので、式変形とかを頑張って追うかは未定。読んでから考えます。

### Road map

全4部構成で、Part1は数学とかlinguisticとかの入門らしい。さすがに飛ばし読みでいいか？

Part2はWord。
ここが一番真面目に読みたい所かな？
collocation、disambiguation、attachment disambiguitiesとかが大切らしい。たぶん知らなさそう。

Part3が文法。
最近あんま見ない話題ね。

Part4が応用。
IRとかテキストのカテゴライズとかの話らしいので、意外と興味深いかも。

# 1章 イントロダクション

序盤の真の文法みたいなルールでは無くただ存在するテキストを確率的に扱うjustificationの所が難しい。
前置詞がどうとかの文法用語にあまり慣れてない上に英語のきわどい使い方についての話が多いので、自分の英語力では判断出来なかったり何を言いたいのか分からなかったりする。

英語力不足は仕方ないのでこの辺は読み飛ばすかなぁ。
そして現代機械学習屋としては、empiricismへの批判というのが良く理解出来ない。
確率モデルは簡単な事しか扱えないと誤解されがちとか言われても、そんな誤解してる奴はもはや居ないのでは…

なお、Corpusはテキストのbodyの事だ、と書いてある。bodyというのは本文という意味かね。
なんとなく語彙の辞書みたいな意味と思ってたが全然違った。

パースの方法とか単語のdisambiguationがどうとか言ってるが、最近のBERTは割とWittgenstein的な単語の意味とか文脈をそのまま学習してて、こういう要素は無い気がする。

こういう一つ一つを正しくやるのが極めて難しい物を積み重ねるって、いかにもあんまうまく行かなさそうだよな。

一回しか出ない単語をhapax legomenaって言うらしい。ギリシャ語。何故。

一回しか出ない単語が多い、という話。
この前見た論文はトリグラムのベクトルで単語を表してたな。bertはこの問題にどうアプローチしてるのかしら？

### Zipf's law

おお、ようやくなんか知ってた方が良さそうな話になってきた。
そうそう、こういう背景知識みたいなの知りたくてこの本読み始めたのだよね。

こういう既知の、言語に関する洞察は知っておくと良い事ありそうたよな。
この周辺の話は面白い。

## 1章を読み終わって

まず最初の印象として、英語の文法の知識が足りないな、という物。
文法用語も知らないし、英語のややこしい用法とかも分からない。
言葉的な事で勝負するならちゃんと英語の文法書とかは勉強した方が良いな、とは思った。
一章が特別そういう記述が多い、というのを期待したいが。

また、現代に役に立たなくなった事も多そうだな、という所。
これから学ぶという時に要らない物を判断する、というのは難しさもあるが、
要らなさそうな所で面倒な所は適度にサボって進めたい。将来何が役に立つかは分からない、という部分はあるが、費用対効果はやはり考えたい。

ただ現代でも役に立つ事も多く載っているな、とも思った。


# 2章 数学

確率論とかの話。シグマ集合体とかがinformalに導入されて、へーとは思ったが、さすがに知らない事は無いかなぁ。


### Example2を解いてみる

簡単なベイズ統計の入門の話題だが、
たまにはやらないとカンが鈍るのでやってみる。
テストが陽性なのをテ、parasitic gapがある事をパと書く。

なお、parasitic gapがなんなのかは知らない。

![images/2018-11-07-195900/0000.jpg]({{"/assets/images/2018-11-07-195900/0000.jpg" | absolute_url}})

うむ、さすがにこの位は解けるな。よしよし。

### エントロピー Example9

いい練習という事でエントロピーの計算をやっておこう。

まずH(C)を出す。

![images/2018-11-07-195900/0001.jpg]({{"/assets/images/2018-11-07-195900/0001.jpg" | absolute_url}})

ふむ、問題無いな。次にV given Cのエントロピーを出そう。

![images/2018-11-07-195900/0002.jpg]({{"/assets/images/2018-11-07-195900/0002.jpg" | absolute_url}})

各場合で1に規格化しないと駄目なのね。

次に本では、H(C, V)を積の法則に相当する式で出している。
まずはこの式を自分でも証明しておこう。

![images/2018-11-07-195900/0003.jpg]({{"/assets/images/2018-11-07-195900/0003.jpg" | absolute_url}})

最後の等号は定義のような物だが、いまいちピンとこないので少し考えておく。

![images/2018-11-07-195900/0004.jpg]({{"/assets/images/2018-11-07-195900/0004.jpg" | absolute_url}})

真ん中の式は中括弧内の期待値になってる。
各Cの値での条件付きエントロピーの期待値になってる訳だ。なるほど。

積の法則に対応するchain ruleは、ようするにCを送ってからCの値を使ったエンコードでVを送る時の平均ビット数、と考えると納得は出来るな。

さて、同時エントロピーはせっかくなので本とは別の方法で導出してみるか。
定義に従い、同時確率で出してみる。

![images/2018-11-07-195900/0005.jpg]({{"/assets/images/2018-11-07-195900/0005.jpg" | absolute_url}})

うむ、特に問題無いな。

### Entropy rate

唐突にサブスクリプトでサブシーケンス表すノーテーションと同時に導入されるので分かりにくいが、単語がn個来る時の、n単語の同時エントロピーをnで割った物か。

同時エントロピーのランダム変数をどんどん増やすとどんな感じかイメージするのは難しいが、何かその極限が表す概念に意味がある事があるのだな。
これは先に出てくるのだろう。

### 2.2.3 Mutual information

初めて見る気がする話だな。ただなんか見覚えはあるので昔どっかでやったのかもしれない。
何にせよ覚えてはいないな。

情報理論っていまだにちゃんとやった事無いんだよな。
そろそろやっても良い頃な気はする。

この本でもCover and Thomasが薦められてるな。Goodfellow本でもこの本が薦められてたのでこれ読むかなぁ。

### 2.2.4 noisy channelと翻訳

おぉ、Encoder-decoderモデルっぽい！
本来はこういう背景を元にRNNで実装したのがEncoder-decoderモデルなんだろうけど。

逆にこういう定式化の汎用性の高さは、汎用noisy channelモデルが出来たらこれだけの応用がある、という事だよな。
それは数年以来に来そうな未来だやね。
楽しみだ。

### 2.2.6 cross entropyの話（が分からない）

式2.48がいまいち分からない。H(L, m)という奴。Lがなんなのか分かってないな。

まず定義式を真面目に見よう。

![images/2018-11-07-195900/0006.jpg]({{"/assets/images/2018-11-07-195900/0006.jpg" | absolute_url}})

良く分からないので、limitを取る前の適当なnについて考えてみよう。

![images/2018-11-07-195900/0007.png]({{"/assets/images/2018-11-07-195900/0007.png" | absolute_url}})

和は何について取ってるか？というと、X1, X2, ... , Xn までのランダム変数のシーケンスだよな。

ややこしいが、各ランダム変数は単語を表す実数値なのだろう。
ようするにこれで単語がn個連なった文を表す訳だな。

pの中もmの中も同時分布を表しているのだろう。
だから前後関係とかは関係ある。

シグマの中を考えてみよう。
pはそんな単語列の文が現れる確率だな。
これはたぶん長さnの文、というのを母集団としててその中で規格化されてるのだろう。

logの中もpだったらこれは単なるエントロピーだな。
それを言語モデルmに置き換えてるのだから、ようするにpとmのクロスエントロピーか。

あー、文とは限らないか。むしろ文と文をつなげて、そういう文が連続すればOKなんだな。とにかくどこまでも続く単語の列を見ていく訳か。

凄く大きなNという数字でこれを考えると、なんかずーっと続く文をN単語目まで見る、をいろんな文書などで繰り返す訳だ。
で、それらの単語の列がそれぞれ現れる可能性をpとして、言語モデルでその単語の列が現れる可能性をmとしてそのクロスエントロピーを出すのだな。

さて、次の2.49は、niceな性質なら成立する、と言ってるがniceってなんやねん。

とりあえず式の意味を考えると、十分に長い一文の同時確率と、十分に長い単語列をいろいろ集めた時の同時確率の期待値が一致する、と言っている。
つまり十分に大きな数Nを固定して、このN個の単語列をたくさん集めると、どの単語列の出てくる確率も一緒と言ってるんだな。

それが何を意味するのかは分からないが、そんな不自然な事でも無い気はする。

追記: あとでこのniceな性質に言及があって、Shanon-McMillan-Breimanの定理と言うらしい。

### 2.2.8 Perplexity

分からない記述があるのでメモを残しておく。
2.53の右辺は以下のように、Hの中は小文字になってる。

![images/2018-11-07-195900/0008.png]({{"/assets/images/2018-11-07-195900/0008.png" | absolute_url}})

こんなのの定義はあったっけ？と前を見直したが見つけられなかった（探し方が不十分かもしれない）。

確率変数に対しての定義は、式2.46にある。
書いておくと、確率分布pに従う確率変数Xと、pとは別のpmfであるqに対して、Xとqのクロスエントロピーとは以下の式で定義される。

![images/2018-11-07-195900/0009.png]({{"/assets/images/2018-11-07-195900/0009.png" | absolute_url}})

これは普通のクロスエントロピーの式だが、Xとqで表記されてるので注意が必要。

さて、式2.53の右辺はこのポイントワイズ版に見える。
2.54式から逆算して考えると定義は以下か。

![images/2018-11-07-195900/0010.png]({{"/assets/images/2018-11-07-195900/0010.png" | absolute_url}})

あ、式2.50の右辺だな。この式にこういうノーテーションを与えたのかな。
これは式2.50によればLとmのクロスエントロピーの近似値だ。

つまり2.53はLとmのクロスエントロピーの近似値とperplexityの関係か。

perplexityがkの時は、基本的に各単語を、毎回「次はこのk通りのうちのどれかだな」と思って見てる事に相当するらしい。

2.54の右辺をk乗して、積の法則で一文字ずつ見ていく時の条件付き確率の積とすればそうなってるな。

## 2章読み終わり。勉強になった！

最初は読む意味無いかな、と思ってたが、Information theory周辺はとても勉強になった。Information theoryやらんとのぅ。

perplexityとか良く評価の所で出てくるが分かってなかったので、今回ちゃんと理解出来て良かった。

2018年現在でも、なかなかこの本は読む価値があるな。続きも楽しみ。

# 3章 Linguistic Essentials

英語の文法の専門的な話で、予想以上に辛い。
話題にしている内容が英語の特殊な用法とかだったりするので、そもそも知らなかったり、知っててもあまり詳しくはない場合もある。

文法用語も知らないのが多く、非ネイティブは不利な分野だなぁ、とか思う。

ただ意外と義務教育でやった英語はこういう話が多く、全く歯が立たない訳でも無い。
英語の文法の参考書とかって向こうの文法書とかを参考に作られてたんだね。当たり前か。

英語の勉強と前向きにとらえて頑張って読む。

### 3.1 品詞とか

Parts of speechと言うらしい。
名詞とか形容詞とか動詞とかをいろいろ説明し、Brown tagでは何が割り当てられているか、を説明している。
細かい分類は良く理解出来ているとは言い難いが、どんなタグが割り振られているかの雰囲気はつかめた。
大変辛かった。

