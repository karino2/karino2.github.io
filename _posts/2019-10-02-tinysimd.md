---
title: Tiny SIMDを作ろう
layout: page
---

あらすじ。

[System Verilogを書くぞ！](https://karino2.github.io/2019/09/19/verilog_intro.html)でmipsを作ったが、せっかくFPGAをいじるならもっと並列度の高い事をやりたい。

という事でおもちゃのSIMDプロセッサを作ろうと思う。これをTiny SIMDと名付ける。

追記： SIMDなのにスレッドあるっておかしくない？と言われて調べたら、これはSIMTと呼ぶのが正しい用語らしい。知らなんだ。レポジトリ作っちゃったのでTinySIMDのままにしておきます。

### 何を作るかを考える

まず作ろうと思ってる物を何も考えずに書き出してみる。

漠然としているが、4スレッドのSIMTを作ろうと思う。
データパスは4つの複製を持つ、という感じで。
目標としてはDRAMにJTAG越しか何かであらかじめ置いた画像データのヒストグラムを計算するプログラムを動かす、くらいにしておこう。

用語としては一つのスレッドをSPと呼ぼう。

コードはROMから読む事にし、DRAMはSRAMとの転送を明示的に行って使う事にする。
だからキャッシュは無しで。


### ブランチ処理を考える

ブランチの処理はSIMTっぽくどっちも実行される、という風にしたいのだが、
アセンブリ的にはどういう動きになるんだろう？
無制限のジャンプでは実現できないような？
考えてみるとCUDAレベルでしか挙動を理解してないな。

for文とif文が出来れば良いので、この命令から考えるか。

先へのbeqは間の命令をnop化する事で全SPで同じpcが使えるな。
そうか。前に戻る時も自分が動くまではずっとnopしてればいいのか。
つまり各SPは次に実行すべきpc（これをpcCandと呼ぼう）を用意していて、自分のpcCandより小さい命令はnopしていけば良いか。

branchとしては各SPのうち、一番小さいものが優先される。
そこからPCPlus4の間は順番に進み、branchの都度4つのSPの一番小さいpcCandが優先される。

おぉ、詳細は考える必要があるが作れそうな気がするな。

ジャンプは全SPが必ず一緒に動かなきゃいけない、という制約をつければいいかな。

### 追加する命令についてざっくり考える

他のSPがSRAMに書き込んだものが見えるようになる為には、バリアが要るな。
単一サイクルならその場で見えるから平気か。

パイプライン的には4回nopすれば良さそうかな。何にせよアセンブリ的には疑似命令でもsyncを足そう。
これを実行するとそれ以後は全SPのSRAMへの書き込み結果が見えるとする。

あとはSPのidが要るな。これは命令というよりはレジスタでいいか。31番をSPのIDとする、とかでいいか。

あとはDRAMからSRAMへの転送が要るな。
イメージ的には各SPが二次元画像の適当な行の範囲をコピーする事でSRAM上にタイルを作り、それを処理したい。
この転送ははみ出した時には勝手に0でも詰めておいて同じ幅をコピーすると割り切ってしまって良い気がする。

DRAMの開始アドレス、SRAMのコピー先アドレスの二つがあればいいか？一応幅も指定するか？
雰囲気としてはlw命令みたいなのでいいよな。

DRAMに戻す場合は端の端数の処理が要るから幅が要る気もするが、その位はパディングしてしまえ、という気もするか。DRAMはどうせ余ってるだろうしな。
SRAMにコピーする場合は幅は要るか。

じゃあimmは幅にするか。


```
d2s $sramaddr, $dramaddr, #width
```

こんな感じか。逆はこうか。

```
s2d $dramaddr, $sramaddr, #width
```


## DRAM回りを調べる

DRAM周辺は良く分かってない。

クロックが違うから注意が必要だよ、と教えてもらう。

通信はシンクロナイザ的なのを挟まないといけないのだろう。
普通はFIFOを挟んで来るまで止まるように書く、とか言われたが全然分からん。

JTAGでDDRに書き込むコードは昔書いたという事で教えてもらう。

[shuntarot/arty-mig: Arty FPGA sample](https://github.com/shuntarot/arty-mig)

まずはこの辺といろいろ聞いた事をググる所からか。


### IP生成回りの資料

とりあえずAXI4でつなげるものらしい、との事だが、これを知らんので、まず以下を読む。（追記：これはハズレ）

[Xilinx: AXI Reference Guide(pdf)](https://www.xilinx.com/support/documentation/ip_documentation/ug761_axi_reference_guide.pdf)

良く分からん。
AXI4のpdfを読むのが先決っぽいが、もうちょっとこう、ゆとりっぽく始められないかなぁ。

お、動画発見。

[https://www.xilinx.com/video/hardware/creating-memory-interface-design-vivado-mig.html](https://www.xilinx.com/video/hardware/creating-memory-interface-design-vivado-mig.html)


なるほど、こういう感じか。さらに同じようなハウツー的なpdfを本家で発見。

[Zynq-7000 AP SoC および 7 シリーズ デバイスメモリ インターフェイス ソリューション v2.3](https://japan.xilinx.com/support/documentation/ip_documentation/mig_7series/v2_3/j_ug586_7Series_MIS.pdf)

だいたい上の動画と同じような内容だが、とにかくこういう感じでIPを作ってつなげる、という事は理解出来た。


### AIXの資料

AMBAの仕様書も読む。リンクがいかにも変わりそうな奴だったので自分がダウンロードした手順を。
まず以下に行く。

[AMBA Specifications – Arm](https://www.arm.com/products/silicon-ip-system/embedded-system-design/amba-specifications)

そしてLatest AMBA Specificationsというリンクをたどり、AMBA 4のAMBA AXI and ACE Protocol Specificationというのを読んでみた。
どうもこれで正解っぽい。

これは大変良く書けていて、最初からこれを読んでおけばよい、というたぐいの奴。
ただちょっと一般的すぎるので、もうちょっと軽くまとまっているのを途中で参照したくなり以下を見る。

[AMBA 3.0に追加された高性能バス用のAXI仕様 ―― チャネル方式を導入し従来のAMBAバスから大きく変更｜Tech Village （テックビレッジ）　／　CQ出版株式会社](http://www.kumikomi.net/archives/2007/08/31amba.php?page=7)

だいたい基本は理解出来たかな。

ただAXIの仕様の上限と実際のDDR3の上限はたぶん一致してないよな。
まずIP生成して叩いてみるのがいいのかなぁ。

### FIFOとかclock domainとか

FIFOとか非同期

- [http://www.sunburst-design.com/papers/CummingsSNUG2002SJ_FIFO1.pdf](http://www.sunburst-design.com/papers/CummingsSNUG2002SJ_FIFO1.pdf)
- [http://www.sunburst-design.com/papers/CummingsSNUG2002SJ_FIFO2.pdf](http://www.sunburst-design.com/papers/CummingsSNUG2002SJ_FIFO2.pdf)

概要は1を読む。実際の実装は2の奴を実装するのが良さそうか？FIFOはこの二つだけで良さそう。

- [https://web.stanford.edu/class/ee183/handouts/synchronization_pres.pdf](https://web.stanford.edu/class/ee183/handouts/synchronization_pres.pdf)

なんかググってたら見つかった講義のスライドだが、10ページ目あたりを見るとどういう物かわかる。

そのほか幾つかyoutubeでググってみてみたが、いまいちなのしか引っかからなかった。

### 必要な物について漠然と考える

FIFOを作ろうと思っていたが、よくよく考えるとDRAMとの転送は全スレッド同時に起こるのだから、CPUは全部止めていいはずだ。
だから必要なのはFIFOじゃなくて、同じSRAMなりBRAMなりをDMAっぽく転送する仕組みだよな。

もっと理想を言えば、lwとかswが来るまではばんばんs2dやd2sを発行して、
バースト長まで行ったらプロセッサ側を止めて転送をする、みたいな挙動がタイルを埋めるなら理想だよなぁ。
それってどのくらい難しいだろう？

むしろDRAMとの送受信は明示的に待つか。dsyncとか作って。
ただバースト長を計算して毎回そこでdsyncを書くのは辛いので、バースト長いっぱいになったらプロセッサは止まって欲しいな。

これならfor文でd2sでタイルを埋めて、dsyncし、以後はsramでいろいろ作業をして、
最後にs2dで結果を戻す、という感じで書けるので、かなりCUDA世代的にも納得のプログラムモデルだな。

ワープに相当するものが一つしか無いので、理想的にはタイルを処理している間に次のタイルをロード出来る方が良い。
イメージ的には、以下のように書ける方が望ましいが、、、

```

for(最初のタイル) {
    d2s();
}
dsync;
for(次のタイル) {
    d2s();
}
for(最初のタイル) {
    最初のタイルの処理
}
dsync;
```

だがこれはプログラムする側的にも複雑すぎるよな。
もっと初歩的に、

```
for(全タイル) {
    // タイルを埋める
    for(今のタイル) {
        d2s();
    }
    dsync;

    // タイルの処理
    for(今のタイル) {
        処理;
    }
    ssync;

    // 結果の書き戻し
    for(今のタイル) {
        s2d();
    }
    dsync;
}
```

くらいで十分か。
なんかだんだんと収拾がつかなくなってきたな。

この辺でとりあえず動かす目標を決めるか。

## 目標とするアセンブリを考える

いろいろとやりたい事が膨らんできてるが、そもそもこれは勉強目的のおもちゃプロセッサだ、という事は忘れてはいけない。
そもそもに掛け算とか剰余とかjalとかjrが無い。
必要に応じて実装しても良いが、学習効果が低いものはなるべくやりたくない。

そこでとりあえず実装する気になるサブセットを考える為に、具体的に動かすものを先に決めよう。

今考えているのは、matmulとヒストグラムの二つ。


### ヒストグラム

疑似コードで書くと以下みたいな感じか？

```
$result_base = 64K-256*4;
$result_cur = $result_base + $tid*256;
for($i = 0; $i < 256; $i++) {
    $sram[$result_cur+$i] = 0;
}

for($block = 0; $block < 640*480/(BARST_WIDTH); $block++) {
    if($tid == 0) {
        $dbase = $block*BARST_WIDTH;
        d2s($0, $dbase, BARST_WIDTH);
    }
    dsync;
    for($idx = 0; $idx < BARST_WIDTH/4; i++) {
        $bin = $sram[$idx*4+$tid]; // 1byteロード
        $sram[$result_cur+$bin] += 1;
    }
}
ssync;

$origin = $result_base+$tid*256/4;
for($i =0; $i < 256/4; $i++) {
    $i = $sram[$origin+$i];
    $i += $sram[$origin+$i+256]
    $i += $sram[$origin+$i+256*2]
    $i += $sram[$origin+$i+256*3]
    $sram[$rogin+$i] = $i;
}
```

結果はdramに書き戻すのが普通だが、今回はホストとか無いのでsramのままでもいい気はする。

対応してないのとしては、4で割るのと掛け算があるな。
掛け算は要るなぁ。
畳み込み出来ない奴は、4と256とBARST_WIDTHを掛けてる奴か。
シフトで行けるか？BARST_WIDTHがまだ分かってないが。

multを実装するのはいいんだが、hiとかloを使う気は無いので、レジスタに下半分入れる感じにする。
定数しか掛けないならmuliでいいか。

右シフト(srl)、muli、lbくらいか。この位なら大きな変更なしで行けそうだな。
いや、lbほんとうに要るか？各スレッドが4バイトずつ処理すればいい、という気もするな。これなら右シフトとandでandはもう実装してある。これでいいか。
じゃあ要るのはsrl, muliの二つだけか。

なかなかSIMTっぽくて、この位動けばいいよな。
1スレッド版と両方作って時間を比べたいね（まぁ転送コストがほぼ全てだろうから違いは出ないか？）


### matmul

とりあえず昔書いたCudaのコード

[Matrix Multiplication for CUDA explanation](https://gist.github.com/karino2/66b3f5e8fc01d6d0ffc44d3bbdf10bca)

を参考に、変更してみる。

```
#define TILE_WIDTH 16

// Compute C = A * B, 
// DRAM上に、行列A, 行列B, 行列Cと連続で置かれているとする。Cは結果。paddingとかは適当。
// 4K境界とかは今は考えない。

// この辺はとりあえず定数とする。あとでDRAMの先頭に置くかも。
int numARows, int numACols,
int numBRows, int numBCols,
int numCRows, int numCCols
int sizeA = numARows * numACols;
int sizeB = numBRows * numBCols;
int numTiles = (numACols - 1)/ TILE_WIDTH + 1;

/*
この三つはsram上で16*16*4バイトの領域。
*/
$tileA, $tileB, $tileC;

// trowとtcolはC上のタイルのインデックス。
for ($trow = 0; $trow < numCRows/TILE_WIDTH;  $trow++) {
    for($tcol = 0; $tcol < numCCols/TILE_WIDTH; $tcol++) {

        /*
        Aは上から$trow番目のタイルを左から右に動かし、Bは左から$tcol番目のタイルを上から下に動かす。
        この動かすインデックスが$tile。
        */
        for ($tile = 0; $tile < numTiles; $tile++)
        {
            /*
            Aの上から$trow番目を横にタイルを埋めていく。横方向に何番目かは$tileが表す。
            */
            for($i =0; i < TILE_WIDTH/4; $i++) {
                $inTileLine = $i*4+$pid;
                $tileLineABegin = ($trow+$inTileLine)*TILE_WIDTH*numACols+$tile*TILE_WIDTH;
                
                d2s(A[$tileLineABegin], $tileA[inTileLine], TILE_WIDTH));
            }
            /*
            端よりはみ出している部分は0で上書き。省略。
            */

            /*
            Bの左から$tcol番目のタイルを下に向かって埋めていく。下方向に何番目かは$tileが表す
            */
            for($i =0; i < TILE_WIDTH/4; $i++) {
                $inTileRow = $i*4+$pid;
                $col = $tcol*TILE_WIDTH+$inTIleCol;
                $row = $tile

                $tileLineBBegin = ($tile*TILE_WIDTH+$inTileRow)*numBCols+$col;
                
                d2s(B[$tileLineBBegin], $tileB[$inTileRow], TILE_WIDTH));
            }
            /*
            ここも同様に端からはみ出た所を0で埋める。省略。
            */

            dsync;
            /*
                $xと$yはタイルの中のインデックス。
            */        
            for($iy = 0; $iy < TILE_WIDTH/4; $iy++) {
                $y = $iy*4+$tid;
                for($ix = 0; $x < TILE_WIDTH; $x++) {
                    for (int i = 0; i < TILE_WIDTH; i++)
                        $tileC[$y][$x] += $tileA[$x][i] * $tileB[i][$y];
                }
            }
        }

        ssync;
        for($iy = 0; $iy < TILE_WIDTH/4; $iy++) {
            $y = $iy*4+$tid;
            $globalY = $trow*TILE_WIDTH+$y;
            if ($globalY < numBCols) {
                s2d(C[$globalY*numBCols],  $tileC[$y*TILE_WIDTH], min(TILE_WIDTH, 端までの幅));
            }
        }

    }
}

```

うーむ、出来たは出来たし、頑張ればこれをフルアセンブリ書くのも出来るとは思うが、
それを動かしてプロセッサの方までデバッグするのは辛いなぁ。
実装的にはレジスタ同士の掛け算、multくらい追加すればいけそうだが。

これはちょっとおもちゃの域を超えてしまっている気もするな。
命令セット的にはこれをサポート出来るような物を考えるが、ここまでの実装はたぶん無理かな。

とりあえずヒストグラムを動かそう。