---
title: "Elements of Information Theoryの三章〜"
layout: page	
---

[Elements of Information Theoryの読書記録](https://karino2.github.io/2019/02/10/143600.html)

[2章だけのノート](https://karino2.github.io/2019/01/31/115955.html)で1Mb越えてしまったのでエントリを分ける。

という事で以下の本の3章から先を読んでいきます。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=B00HLG9ISQ&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

# 3 章 Asymptotic equipartition property

日本語にすると漸近的等分配の法則、か？

内容としては、iidな確率変数をたくさんサンプリングすると、その同時確率のlogはHの-n倍になる、という物。

ベルヌーイ試行での例が書いてある。
n個のサンプルを取る。
その組み合わせによる実現しやすさは、もちろんデータの組み合わせごとに異なる。

だが、「n個のサンプルをとって同時確率を求めた値」は、高確率で2の-nH乗に近い、との事。

近いとはどういう意味か？とかいう気もするが、そこはおいといて、この法則の意味する所が良く分からないな。
なんか学部生の頃等分配の法則って統計力学とかでやった気がするが全然分からなかった思い出が蘇ってきた。

ほとんど表しか出ないコインを投げる。
エントロピーはほとんど0となるが、ちょっとだけ正の数だ。

すると、n回コインを投げたらだいたい全部表だよな。
その確率はほとんど1だが、nを大きくしていくとちょっとずつ下がる。

表の確率をpとして0.99としよう。
10回全部表の確率は0.99の10乗となる。

式3.1を考えてみよう。
ここから少し乖離した事象とはなんだろう？
それは一つだけ裏が出る、という事象だよな。

つまり3.1が言ってるのは、全部表と一枚だけ裏、でだいたい起こる事の全部が含まれるよ、と言っているのか。

### 確率変数の収束

以下の3つの収束の定義が載ってる。

![images/2019-02-10-144121/0000.jpg]({{"/assets/images/2019-02-10-144121/0000.jpg" | absolute_url}})

他の本も見直しておこう。

[Dudley](https://karino2.github.io/2018/03/18/167.html)
ではp261で1番目と3番目の定義がある。
Dudleyでは、確率1で起こる事象をalmost surelyに起こると言う、と言っていて、YnがYに収束する確率が1なら、 converge a.s.と言っている。

もう一冊、共立出版の「ルベーグ積分から確率論」も持ってるので見ておこう。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4320015622&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

共立出版の方では、p117でa.s.の定義があるな。
こちらは「ほとんど確実に成立する」という言葉を使ってる。

in probabilityの方はp122で、確率収束、という言葉で紹介している。
また次のページに、p次平均収束というのも載ってる。書き方は違うが本書のin mean squareと同じ話っほい（こちらの定義はLpノルムによるバナッハ空間、という話にちょっと触れているのでもうちょっと厳密か）。

## 3.1 AEP定理

定理は単純に、独立なら同時確率の積になるので、logの同時確率が積になって和になって大数の法則で証明出来る。
自分で少し考えた時もだいたいそういう事だな、と思ったのでこの証明は納得出来る。

次にtypical setの定義がある。

### typical set

typical setの定義は以下。

![images/2019-02-10-144121/0001.jpg]({{"/assets/images/2019-02-10-144121/0001.jpg" | absolute_url}})

### 定理3.2.1 コーディングの平均長

導出のところで以下のイプシロンダッシュが任意に小さく出来る、とあるが、分からん。

![images/2019-02-10-144121/0002.png]({{"/assets/images/2019-02-10-144121/0002.png" | absolute_url}})

logのXってnを大きくすると当然大きくなる。
だいたいlog nに近い感じで大きくなるのでは？

するとそれにイプシロンを掛けた物は、nを大きくしていくと小さくなるか？は全然自明じゃない気がする。
逆に任意のaに対して、大きいn を持ってくればイプシロンダッシュをaより大きく出来てしまうのでは？

いや、良く見るとこのギリシャ文字のXは、nがついてないな。この定義はなんだろう？
あぁ、一個のランダム変数Xの濃度か。それがn個並んでるからn logなのか。なるほど。
確かにこの濃度はnには依存しないな。

これがイプシロンで抑えられるのか。なるほど。理解した。

良く噂で聞いてたコーディング長の話はこういう奴なのか。クリアで面白いね。

