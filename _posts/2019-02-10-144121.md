---
title: "Elements of Information Theoryの三章〜"
layout: page	
---

[Elements of Information Theoryの読書記録](https://karino2.github.io/2019/02/10/143600.html)

[2章だけのノート](https://karino2.github.io/2019/01/31/115955.html)で1Mb越えてしまったのでエントリを分ける。

という事で以下の本の3章から先を読んでいきます。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=B00HLG9ISQ&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

# 3 章 Asymptotic equipartition property

日本語にすると漸近的等分配の法則、か？

内容としては、iidな確率変数をたくさんサンプリングすると、その同時確率のlogはHの-n倍になる、という物。

ベルヌーイ試行での例が書いてある。
n個のサンプルを取る。
その組み合わせによる実現しやすさは、もちろんデータの組み合わせごとに異なる。

だが、「n個のサンプルをとって同時確率を求めた値」は、高確率で2の-nH乗に近い、との事。

近いとはどういう意味か？とかいう気もするが、そこはおいといて、この法則の意味する所が良く分からないな。
なんか学部生の頃等分配の法則って統計力学とかでやった気がするが全然分からなかった思い出が蘇ってきた。

ほとんど表しか出ないコインを投げる。
エントロピーはほとんど0となるが、ちょっとだけ正の数だ。

すると、n回コインを投げたらだいたい全部表だよな。
その確率はほとんど1だが、nを大きくしていくとちょっとずつ下がる。

表の確率をpとして0.99としよう。
10回全部表の確率は0.99の10乗となる。

式3.1を考えてみよう。
ここから少し乖離した事象とはなんだろう？
それは一つだけ裏が出る、という事象だよな。

つまり3.1が言ってるのは、全部表と一枚だけ裏、でだいたい起こる事の全部が含まれるよ、と言っているのか。

### 確率変数の収束

以下の3つの収束の定義が載ってる。

![images/2019-02-10-144121/0000.jpg]({{"/assets/images/2019-02-10-144121/0000.jpg" | absolute_url}})

他の本も見直しておこう。

[Dudley](https://karino2.github.io/2018/03/18/167.html)
ではp261で1番目と3番目の定義がある。
Dudleyでは、確率1で起こる事象をalmost surelyに起こると言う、と言っていて、YnがYに収束する確率が1なら、 converge a.s.と言っている。

もう一冊、共立出版の「ルベーグ積分から確率論」も持ってるので見ておこう。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4320015622&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

共立出版の方では、p117でa.s.の定義があるな。
こちらは「ほとんど確実に成立する」という言葉を使ってる。

in probabilityの方はp122で、確率収束、という言葉で紹介している。
また次のページに、p次平均収束というのも載ってる。書き方は違うが本書のin mean squareと同じ話っほい（こちらの定義はLpノルムによるバナッハ空間、という話にちょっと触れているのでもうちょっと厳密か）。

## 3.1 AEP定理

定理は単純に、独立なら同時確率の積になるので、logの同時確率が積になって和になって大数の法則で証明出来る。
自分で少し考えた時もだいたいそういう事だな、と思ったのでこの証明は納得出来る。

次にtypical setの定義がある。

### typical set

typical setの定義は以下。

![images/2019-02-10-144121/0001.jpg]({{"/assets/images/2019-02-10-144121/0001.jpg" | absolute_url}})

何も書いてないが、i.i.dかな。大文字のXはこの一つのxに対応する確率変数か。

### 定理3.2.1 コーディングの平均長

導出のところで以下のイプシロンダッシュが任意に小さく出来る、とあるが、分からん。

![images/2019-02-10-144121/0002.png]({{"/assets/images/2019-02-10-144121/0002.png" | absolute_url}})

logのXってnを大きくすると当然大きくなる。
だいたいlog nに近い感じで大きくなるのでは？

するとそれにイプシロンを掛けた物は、nを大きくしていくと小さくなるか？は全然自明じゃない気がする。
逆に任意のaに対して、大きいn を持ってくればイプシロンダッシュをaより大きく出来てしまうのでは？

いや、良く見るとこのギリシャ文字のXは、nがついてないな。この定義はなんだろう？
あぁ、一個のランダム変数Xの濃度か。それがn個並んでるからn logなのか。なるほど。
確かにこの濃度はnには依存しないな。

これがイプシロンで抑えられるのか。なるほど。理解した。

良く噂で聞いてたコーディング長の話はこういう奴なのか。クリアで面白いね。

### 3.2までの自分の理解

typical setは、だいたいそれが出る、という系列を表していて、数はまぁまぁ少ない、という性質がある。
特にnを増やしていくとほぼその中に含まれる確率は1となる。

このtypical setの数は少ないので、それをコーディング出来る長さと、typical setかどうかを識別するビットでだいたいは平均のコーディング長となる。

コーディングうんぬんはおいといて、このtypical setの濃度は低くてしかも確率はだいたい1、というのが面白い性質だね。
これはいろいろ使えそうな気もする。

# 4章 Entropy rates

さらさらと読んでいけるので、理解するだけならあまりメモする必要も感じないが、そうするとあとで必要になった時に困りそう。
という事であとで使いそうな範囲でメモするか。

4章は確率過程なのでi.i.dじゃなくて依存がある時の話。

**Stationaryの定義**

確率過程がstationaryとは、以下の性質を満たす事である。

![images/2019-02-10-144121/0003.jpg]({{"/assets/images/2019-02-10-144121/0003.jpg" | absolute_url}})

特定の系列のでやすさは、開始時点に依存しない、という事かね。
任意のlで成り立つ、というのは結構強いな。こんな強かったっけ？あんまり覚えてないな。

**4.2 Entropy rate**

エントロピーrate の定義は以下。

![images/2019-02-10-144121/0004.png]({{"/assets/images/2019-02-10-144121/0004.png" | absolute_url}})

また、以下の極限も存在して、両者は一致するらしい。

![images/2019-02-10-144121/0005.jpg]({{"/assets/images/2019-02-10-144121/0005.jpg" | absolute_url}})

極限が存在するのは非負な減少列だから。
で、極限が存在するものの平均は極限になる、みたいなのと合わせて証明出来る。
感覚的にも積の法則でバラせばまぁまぁ納得出来る。

### 4.2.4 stationary  markov chain

証明が理解出来なかったので自分でやってみる。
stationaryなマルコフ連鎖なら、真ん中の項になるまでは、少し上にわかりやすい途中式がある。

![images/2019-02-10-144121/0006.jpg]({{"/assets/images/2019-02-10-144121/0006.jpg" | absolute_url}})

この最後の条件付きエントロピーを計算してみよう。

![images/2019-02-10-144121/0007.jpg]({{"/assets/images/2019-02-10-144121/0007.jpg" | absolute_url}})

お、大した事無かった。Pijが条件付き確率になるのか。
まぁこういうのはたまに定義に戻って計算しないと分からなくなるので、こうやって計算するのは必要な事だ。

## 4章は簡単だがなかなか面白かった

さらさらと読み終わってしまい別段難しい事も無かったが、なかなかおもしろかった。特に最後のマルコフ連鎖の関数のentropy rateのboundを考えるのは簡単な計算なのにこんな一般的な事に面白い結論がでる、というのは驚きだ。

個人的には、この本はそこまで直接的な興味が無い所はさらさら流していくのがいいんじゃないか、と思った。
頑張って全部理解するよりは、サラサラ流して、もう一冊のMacKayの本も読んで見る方が良い気がする。

# 5章 Data Compression

ここは教養としてサラサラ読もう。
メモをとるのもサボりたいが、定義がいっぱい出てきて辛くなってきたので読み進めるのに必要な程度メモる。

**Extension**

Xを有限個並べた列からDの有限の長さへのマッピングで、以下のようなものをCode Cのextensionと呼ぶ。

![images/2019-02-10-144121/0008.jpg]({{"/assets/images/2019-02-10-144121/0008.jpg" | absolute_url}})

**nonsingular**

以下のコードを、nonsingularと言う。

![images/2019-02-10-144121/0009.png]({{"/assets/images/2019-02-10-144121/0009.png" | absolute_url}})

extensionがnonsingularな事をuniquely decodableと呼ぶとか。

普通に考えたらextensionじゃないコードでもuniquely decodableなものなんてありそうだが、そういう時は呼ばないのかな？ほんまかいな。

**instantaneous code**

どのcodewordも他のcodewordのprefixになってない事。

prefix code とも呼ぶらしい。どっちかというとno prefix codeと呼ぶべきもののような気もするが…

定理5.3.1はcが1より大きいじゃなくて1/cが1より大きい、だね。

